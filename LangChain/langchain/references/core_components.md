# Langchain - Core Components

**Pages:** 3

---

## å·¥å…·

**URL:** https://langchain-doc.cn/v1/python/langchain/tools.html

å·¥å…·è®¸å¤š AI åº”ç”¨ç¨‹åºé€šè¿‡è‡ªç„¶è¯­è¨€ä¸ç”¨æˆ·äº¤äº’ã€‚ç„¶è€Œï¼ŒæŸäº›ç”¨ä¾‹è¦æ±‚æ¨¡å‹ä½¿ç”¨ç»“æ„åŒ–è¾“å…¥ç›´æ¥ä¸å¤–éƒ¨ç³»ç»Ÿï¼ˆä¾‹å¦‚ APIã€æ•°æ®åº“ æˆ– æ–‡ä»¶ç³»ç»Ÿï¼‰å¯¹æ¥ã€‚å·¥å…· æ˜¯ ä»£ç†ï¼ˆagentsï¼‰ è°ƒç”¨æ¥æ‰§è¡Œæ“ä½œçš„ç»„ä»¶ã€‚å®ƒä»¬é€šè¿‡å…è®¸æ¨¡å‹é€šè¿‡å®šä¹‰æ˜ç¡®çš„è¾“å…¥å’Œè¾“å‡ºä¸ä¸–ç•Œäº¤äº’æ¥æ‰©å±•æ¨¡å‹çš„åŠŸèƒ½ã€‚å·¥å…·å°è£…äº†ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°åŠå…¶è¾“å…¥æ¶æ„ï¼ˆschemaï¼‰ã€‚è¿™äº›å¯ä»¥ä¼ é€’ç»™å…¼å®¹çš„ èŠå¤©æ¨¡å‹ï¼ˆchat modelsï¼‰ï¼Œè®©æ¨¡å‹å†³å®šæ˜¯å¦ä»¥åŠä½¿ç”¨ä»€ä¹ˆå‚æ•°æ¥è°ƒç”¨å·¥å…·ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå·¥å…·è°ƒç”¨ ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆç¬¦åˆæŒ‡å®šè¾“å…¥æ¶æ„çš„è¯·æ±‚ã€‚æœåŠ¡å™¨ç«¯å·¥å…·ä½¿ç”¨ (Server-side tool use)æŸäº›èŠå¤©æ¨¡å‹ï¼ˆä¾‹å¦‚ OpenAIã€Anthropic å’Œ Geminiï¼‰å…·æœ‰ å†…ç½®å·¥å…·ï¼Œè¿™äº›å·¥å…·åœ¨æœåŠ¡å™¨ç«¯æ‰§è¡Œï¼Œä¾‹å¦‚ç½‘ç»œæœç´¢å’Œä»£ç è§£é‡Šå™¨ã€‚è¯·å‚é˜… æä¾›å•†æ¦‚è§ˆï¼ˆprovider overviewï¼‰ äº†è§£å¦‚ä½•ä½¿ç”¨æ‚¨çš„ç‰¹å®šèŠå¤©æ¨¡å‹è®¿é—®è¿™äº›å·¥å…·ã€‚åˆ›å»ºå·¥å…· (Create tools)åŸºæœ¬å·¥å…·å®šä¹‰ (Basic tool definition)åˆ›å»ºå·¥å…·æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨ @tool è£…é¥°å™¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå‡½æ•°çš„ æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆdocstringï¼‰ä¼šæˆä¸ºå·¥å…·çš„æè¿°ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£ä½•æ—¶ä½¿ç”¨å®ƒï¼šfrom langchain.tools import tool

@tool
def search_database(query: str, limit: int = 10) -> str:
    """Search the customer database for records matching the query.

    Args:
        query: Search terms to look for
        limit: Maximum number of results to return
    """
    return f"Found {limit} results for '{query}'"ç±»å‹æç¤º æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå®ƒä»¬å®šä¹‰äº†å·¥å…·çš„è¾“å…¥æ¶æ„ã€‚æ–‡æ¡£å­—ç¬¦ä¸²åº”è¯¥ä¿¡æ¯ä¸°å¯Œä¸”ç®€æ´ï¼Œä»¥å¸®åŠ©æ¨¡å‹ç†è§£å·¥å…·çš„ç”¨é€”ã€‚è‡ªå®šä¹‰å·¥å…·å±æ€§ (Customize tool properties)è‡ªå®šä¹‰å·¥å…·åç§° (Custom tool name)é»˜è®¤æƒ…å†µä¸‹ï¼Œå·¥å…·åç§°æ¥è‡ªå‡½æ•°åç§°ã€‚å½“æ‚¨éœ€è¦æ›´å…·æè¿°æ€§çš„åç§°æ—¶ï¼Œå¯ä»¥è¦†ç›–å®ƒï¼š@tool("web_search")  # Custom name
def search(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

print(search.name)  # web_searchè‡ªå®šä¹‰å·¥å…·æè¿° (Custom tool description)è¦†ç›–è‡ªåŠ¨ç”Ÿæˆçš„å·¥å…·æè¿°ï¼Œä»¥æä¾›æ›´æ¸…æ™°çš„æ¨¡å‹æŒ‡å¯¼ï¼š@tool("calculator", description="Performs arithmetic calculations. Use this for any math problems.")
def calc(expression: str) -> str:
    """Evaluate mathematical expressions."""
    return str(eval(expression))é«˜çº§æ¶æ„å®šä¹‰ (Advanced schema definition)ä½¿ç”¨ Pydantic æ¨¡å‹ æˆ– JSON æ¶æ„ å®šä¹‰å¤æ‚çš„è¾“å…¥ï¼šfrom pydantic import BaseModel, Field
from typing import Literal

class WeatherInput(BaseModel):
    """Input for weather queries."""
    location: str = Field(description="City name or coordinates")
    units: Literal["celsius", "fahrenheit"] = Field(
        default="celsius",
        description="Temperature unit preference"
    )
    include_forecast: bool = Field(
        default=False,
        description="Include 5-day forecast"
    )

@tool(args_schema=WeatherInput)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return resultweather_schema = {
    "type": "object",
    "properties": {
        "location": {"type": "string"},
        "units": {"type": "string"},
        "include_forecast": {"type": "boolean"}
    },
    "required": ["location", "units", "include_forecast"]
}

@tool(args_schema=weather_schema)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return resultè®¿é—®ä¸Šä¸‹æ–‡ (Accessing Context)ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼š å½“å·¥å…·å¯ä»¥è®¿é—®ä»£ç†çŠ¶æ€ã€è¿è¡Œæ—¶ä¸Šä¸‹æ–‡å’Œé•¿æœŸè®°å¿†æ—¶ï¼Œå®ƒä»¬çš„åŠŸèƒ½æœ€å¼ºå¤§ã€‚è¿™ä½¿å¾—å·¥å…·èƒ½å¤Ÿåšå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†³ç­–ã€ä¸ªæ€§åŒ–å“åº”å¹¶åœ¨å¯¹è¯ä¸­ç»´æŠ¤ä¿¡æ¯ã€‚å·¥å…·å¯ä»¥é€šè¿‡ ToolRuntime å‚æ•°è®¿é—®è¿è¡Œæ—¶ä¿¡æ¯ï¼Œè¯¥å‚æ•°æä¾›ï¼šStateï¼ˆçŠ¶æ€ï¼‰ - æµç»æ‰§è¡Œçš„å¯å˜æ•°æ®ï¼ˆæ¶ˆæ¯ã€è®¡æ•°å™¨ã€è‡ªå®šä¹‰å­—æ®µï¼‰Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰ - ä¸å¯å˜çš„é…ç½®ï¼Œå¦‚ç”¨æˆ· IDã€ä¼šè¯è¯¦ç»†ä¿¡æ¯æˆ–ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„é…ç½®Storeï¼ˆå­˜å‚¨ï¼‰ - è·¨å¯¹è¯çš„æŒä¹…é•¿æœŸè®°å¿†Stream Writerï¼ˆæµå†™å…¥å™¨ï¼‰ - åœ¨å·¥å…·æ‰§è¡Œæ—¶æµå¼ä¼ è¾“è‡ªå®šä¹‰æ›´æ–°Configï¼ˆé…ç½®ï¼‰ - æ‰§è¡Œçš„ RunnableConfigTool Call IDï¼ˆå·¥å…·è°ƒç”¨ IDï¼‰ - å½“å‰å·¥å…·è°ƒç”¨çš„ IDToolRuntimeä½¿ç”¨ ToolRuntime åœ¨ä¸€ä¸ªå‚æ•°ä¸­è®¿é—®æ‰€æœ‰è¿è¡Œæ—¶ä¿¡æ¯ã€‚åªéœ€å°† runtime: ToolRuntime æ·»åŠ åˆ°æ‚¨çš„å·¥å…·ç­¾åä¸­ï¼Œå®ƒå°±ä¼šè¢«è‡ªåŠ¨æ³¨å…¥ï¼Œè€Œä¸ä¼šæš´éœ²ç»™ LLMã€‚ToolRuntime: ä¸€ä¸ªç»Ÿä¸€çš„å‚æ•°ï¼Œä¸ºå·¥å…·æä¾›å¯¹çŠ¶æ€ã€ä¸Šä¸‹æ–‡ã€å­˜å‚¨ã€æµå¼ä¼ è¾“ã€é…ç½®å’Œå·¥å…·è°ƒç”¨ ID çš„è®¿é—®ã€‚è¿™å–ä»£äº†ä½¿ç”¨å•ç‹¬çš„ InjectedStateã€InjectedStoreã€get_runtime å’Œ InjectedToolCallId æ³¨é‡Šçš„æ—§æ¨¡å¼ã€‚è®¿é—®çŠ¶æ€ï¼šå·¥å…·å¯ä»¥ä½¿ç”¨ ToolRuntime è®¿é—®å½“å‰çš„å›¾çŠ¶æ€ï¼šfrom langchain.tools import tool, ToolRuntime

# Access the current conversation state
@tool
def summarize_conversation(
    runtime: ToolRuntime
) -> str:
    """Summarize the conversation so far."""
    messages = runtime.state["messages"]

    human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")

    return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

# Access custom state fields
@tool
def get_user_preference(
    pref_name: str,
    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model
) -> str:
    """Get a user preference value."""
    preferences = runtime.state.get("user_preferences", {})
    return preferences.get(pref_name, "Not set")tool_runtime å‚æ•°å¯¹æ¨¡å‹æ˜¯éšè—çš„ã€‚å¯¹äºä¸Šé¢çš„ç¤ºä¾‹ï¼Œæ¨¡å‹åœ¨å·¥å…·æ¶æ„ä¸­åªçœ‹åˆ° pref_name - tool_runtime ä¸åŒ…å«åœ¨è¯·æ±‚ä¸­ã€‚æ›´æ–°çŠ¶æ€ï¼šä½¿ç”¨ Command æ¥æ›´æ–°ä»£ç†çš„çŠ¶æ€æˆ–æ§åˆ¶å›¾çš„æ‰§è¡Œæµç¨‹ï¼šfrom langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

# Update the conversation history by removing all messages
@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""

    return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

# Update the user_name in the agent state
@tool
def update_user_name(
    new_name: str,
    runtime: ToolRuntime
) -> Command:
    """Update the user's name."""
    return Command(update={"user_name": new_name})ä¸Šä¸‹æ–‡ (Context)é€šè¿‡ runtime.context è®¿é—®ä¸å¯å˜çš„é…ç½®å’Œä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¾‹å¦‚ç”¨æˆ· IDã€ä¼šè¯è¯¦ç»†ä¿¡æ¯æˆ–ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„é…ç½®ã€‚å·¥å…·å¯ä»¥é€šè¿‡ ToolRuntime è®¿é—®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼šfrom dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime


USER_DATABASE = {
    "user123": {
        "name": "Alice Johnson",
        "account_type": "Premium",
        "balance": 5000,
        "email": "alice@example.com"
    },
    "user456": {
        "name": "Bob Smith",
        "account_type": "Standard",
        "balance": 1200,
        "email": "bob@example.com"
    }
}

@dataclass
class UserContext:
    user_id: str

@tool
def get_account_info(runtime: ToolRuntime[UserContext]) -> str:
    """Get the current user's account information."""
    user_id = runtime.context.user_id

    if user_id in USER_DATABASE:
        user = USER_DATABASE[user_id]
        return f"Account holder: {user['name']}\nType: {user['account_type']}\nBalance: ${user['balance']}"
    return "User not found"

model = ChatOpenAI(model="gpt-4o")
agent = create_agent(
    model,
    tools=[get_account_info],
    context_schema=UserContext,
    system_prompt="You are a financial assistant."
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "What's my current balance?"}]},
    context=UserContext(user_id="user123")
)å†…å­˜ï¼ˆå­˜å‚¨ï¼‰(Memory (Store))ä½¿ç”¨ å­˜å‚¨ï¼ˆstoreï¼‰ è®¿é—®è·¨å¯¹è¯çš„æŒä¹…æ•°æ®ã€‚é€šè¿‡ runtime.store è®¿é—®å­˜å‚¨ï¼Œå®ƒå…è®¸æ‚¨ä¿å­˜å’Œæ£€ç´¢ç‰¹å®šäºç”¨æˆ·æˆ–ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„æ•°æ®ã€‚å·¥å…·å¯ä»¥é€šè¿‡ ToolRuntime è®¿é—®å’Œæ›´æ–°å­˜å‚¨ï¼šfrom typing import Any
from langgraph.store.memory import InMemoryStore
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime


# Access memory
@tool
def get_user_info(user_id: str, runtime: ToolRuntime) -> str:
    """Look up user info."""
    store = runtime.store
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

# Update memory
@tool
def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:
    """Save user info."""
    store = runtime.store
    store.put(("users",), user_id, user_info)
    return "Successfully saved user info."

store = InMemoryStore()
agent = create_agent(
    model,
    tools=[get_user_info, save_user_info],
    store=store
)

# First session: save user info
agent.invoke({
    "messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

# Second session: get user info
agent.invoke({
    "messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})
# Here is the user info for user with ID "abc123":
# - Name: Foo
# - Age: 25
# - Email: foo@langchain.devæµå†™å…¥å™¨ (Stream Writer)ä½¿ç”¨ runtime.stream_writer åœ¨å·¥å…·æ‰§è¡Œæ—¶æµå¼ä¼ è¾“è‡ªå®šä¹‰æ›´æ–°ã€‚è¿™å¯¹äºå‘ç”¨æˆ·æä¾›æœ‰å…³å·¥å…·æ­£åœ¨åšä»€ä¹ˆçš„å®æ—¶åé¦ˆå¾ˆæœ‰ç”¨ã€‚from langchain.tools import tool, ToolRuntime

@tool
def get_weather(city: str, runtime: ToolRuntime) -> str:
    """Get weather for a given city."""
    writer = runtime.stream_writer

    # Stream custom updates as the tool executes
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")

    return f"It's always sunny in {city}!"å¦‚æœæ‚¨åœ¨å·¥å…·ä¸­ä½¿ç”¨ runtime.stream_writerï¼Œåˆ™å¿…é¡»åœ¨ LangGraph æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­è°ƒç”¨è¯¥å·¥å…·ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… æµå¼ä¼ è¾“ï¼ˆStreamingï¼‰ã€‚æœ€è¿‘æ›´æ–°2025/10/25 19:51ä¸Šä¸€é¡µæ¶ˆæ¯ä¸‹ä¸€é¡µçŸ­æœŸè®°å¿†

---

## æ¨¡å‹

**URL:** https://langchain-doc.cn/v1/python/langchain/models.html

æ¨¡å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ æ˜¯å¼ºå¤§çš„ AI å·¥å…·ï¼Œèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ç†è§£å’Œç”Ÿæˆæ–‡æœ¬ã€‚å®ƒä»¬ç”¨é€”å¹¿æ³›ï¼Œå¯ç”¨äºæ’°å†™å†…å®¹ã€ç¿»è¯‘è¯­è¨€ã€æ€»ç»“ä¿¡æ¯å’Œå›ç­”é—®é¢˜ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯é¡¹ä»»åŠ¡è¿›è¡Œä¸“é—¨è®­ç»ƒã€‚é™¤äº†æ–‡æœ¬ç”Ÿæˆä¹‹å¤–ï¼Œè®¸å¤šæ¨¡å‹è¿˜æ”¯æŒï¼šå·¥å…·è°ƒç”¨ - è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æ•°æ®åº“æŸ¥è¯¢æˆ– API è°ƒç”¨ï¼‰å¹¶å°†ç»“æœç”¨äºå“åº”ä¸­ã€‚ç»“æ„åŒ–è¾“å‡º - æ¨¡å‹çš„å“åº”è¢«é™åˆ¶ä¸ºéµå¾ªå®šä¹‰çš„æ ¼å¼ã€‚å¤šæ¨¡æ€ - å¤„ç†å’Œè¿”å›éæ–‡æœ¬æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚æ¨ç† - æ¨¡å‹æ‰§è¡Œå¤šæ­¥æ¨ç†ä»¥å¾—å‡ºç»“è®ºã€‚æ¨¡å‹æ˜¯ ä»£ç† çš„æ¨ç†å¼•æ“ã€‚å®ƒä»¬é©±åŠ¨ä»£ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œå†³å®šè°ƒç”¨å“ªäº›å·¥å…·ã€å¦‚ä½•è§£é‡Šç»“æœä»¥åŠä½•æ—¶æä¾›æœ€ç»ˆç­”æ¡ˆã€‚æ‚¨é€‰æ‹©çš„æ¨¡å‹çš„è´¨é‡å’Œèƒ½åŠ›ç›´æ¥å½±å“ä»£ç†çš„å¯é æ€§å’Œæ€§èƒ½ã€‚ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²â€”â€”æœ‰äº›æ›´æ“…é•¿éµå¾ªå¤æ‚æŒ‡ä»¤ï¼Œæœ‰äº›æ›´æ“…é•¿ç»“æ„åŒ–æ¨ç†ï¼Œæœ‰äº›æ”¯æŒæ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ä»¥å¤„ç†æ›´å¤šä¿¡æ¯ã€‚LangChain çš„æ ‡å‡†æ¨¡å‹æ¥å£ä¸ºæ‚¨æä¾›å¯¹ä¼—å¤šä¸åŒæä¾›å•†é›†æˆçš„è®¿é—®ï¼Œè¿™ä½¿å¾—å®éªŒå’Œåˆ‡æ¢æ¨¡å‹ä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨ç”¨ä¾‹çš„æ¨¡å‹å˜å¾—éå¸¸å®¹æ˜“ã€‚ä¿¡æ¯ æœ‰å…³ç‰¹å®šæä¾›å•†çš„é›†æˆä¿¡æ¯å’ŒåŠŸèƒ½ï¼Œè¯·å‚é˜…æä¾›å•†çš„é›†æˆé¡µé¢ã€‚åŸºæœ¬ç”¨æ³•æ¨¡å‹å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼ä½¿ç”¨ï¼šä¸ä»£ç†ä¸€èµ·ä½¿ç”¨ - åˆ›å»ºä»£ç†æ—¶å¯åŠ¨æ€æŒ‡å®šæ¨¡å‹ã€‚ç‹¬ç«‹ä½¿ç”¨ - æ¨¡å‹å¯ç›´æ¥è°ƒç”¨ï¼ˆåœ¨ä»£ç†å¾ªç¯ä¹‹å¤–ï¼‰ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»æˆ–æå–ç­‰ä»»åŠ¡ï¼Œè€Œæ— éœ€ä»£ç†æ¡†æ¶ã€‚åŒä¸€æ¨¡å‹æ¥å£åœ¨ä¸¤ç§ä¸Šä¸‹æ–‡ä¸­å‡é€‚ç”¨ï¼Œè¿™ä¸ºæ‚¨æä¾›äº†ä»ç®€å•å¼€å§‹å¹¶æ ¹æ®éœ€è¦æ‰©å±•åˆ°æ›´å¤æ‚åŸºäºä»£ç†çš„å·¥ä½œæµç¨‹çš„çµæ´»æ€§ã€‚åˆå§‹åŒ–æ¨¡å‹åœ¨ LangChain ä¸­å¼€å§‹ä½¿ç”¨ç‹¬ç«‹æ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ init_chat_model ä»æ‚¨é€‰æ‹©çš„æä¾›å•†åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼ˆä»¥ä¸‹ç¤ºä¾‹ï¼‰ï¼šOpenAIğŸ‘‰ é˜…è¯» OpenAI èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£pip install -U "langchain[openai]"import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("openai:gpt-4.1")import os
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "sk-..."

model = ChatOpenAI(model="gpt-4.1")AnthropicğŸ‘‰ é˜…è¯» Anthropic èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£pip install -U "langchain[anthropic]"import os
from langchain.chat_models import init_chat_model

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("anthropic:claude-sonnet-4-5")import os
from langchain_anthropic import ChatAnthropic

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = ChatAnthropic(model="claude-sonnet-4-5")AzureğŸ‘‰ é˜…è¯» Azure èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£pip install -U "langchain[openai]"import os
from langchain.chat_models import init_chat_model

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
    "azure_openai:gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)import os
from langchain_openai import AzureChatOpenAI

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = AzureChatOpenAI(
    model="gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]
)Google GeminiğŸ‘‰ é˜…è¯» Google GenAI èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£pip install -U "langchain[google-genai]"import os
from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")import os
from langchain_google_genai import ChatGoogleGenerativeAI

os.environ["GOOGLE_API_KEY"] = "..."

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")AWS BedrockğŸ‘‰ é˜…è¯» AWS Bedrock èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£pip install -U "langchain[aws]"from langchain.chat_models import init_chat_model

# è¯·æŒ‰ç…§æ­¤å¤„çš„æ­¥éª¤é…ç½®æ‚¨çš„å‡­æ®ï¼š
# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

model = init_chat_model(
    "anthropic.claude-3-5-sonnet-20240620-v1:0",
    model_provider="bedrock_converse",
)from langchain_aws import ChatBedrock

model = ChatBedrock(model="anthropic.claude-3-5-sonnet-20240620-v1:0")response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰ä¼šè¯´è¯ï¼Ÿ")æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¦‚ä½•ä¼ é€’æ¨¡å‹å‚æ•°ï¼Œè¯·å‚é˜… init_chat_modelã€‚å…³é”®æ–¹æ³•æ–¹æ³•è¯´æ˜Invokeæ¨¡å‹æ¥å—æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨ç”Ÿæˆå®Œæ•´å“åº”åè¾“å‡ºæ¶ˆæ¯ã€‚Streamè°ƒç”¨æ¨¡å‹ï¼Œä½†å®æ—¶æµå¼ä¼ è¾“ç”Ÿæˆçš„è¾“å‡ºã€‚Batchå°†å¤šä¸ªè¯·æ±‚æ‰¹é‡å‘é€ç»™æ¨¡å‹ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å¤„ç†ã€‚ä¿¡æ¯ é™¤äº†èŠå¤©æ¨¡å‹ä¹‹å¤–ï¼ŒLangChain è¿˜æ”¯æŒå…¶ä»–ç›¸å…³æŠ€æœ¯ï¼Œå¦‚åµŒå…¥æ¨¡å‹å’Œå‘é‡å­˜å‚¨ã€‚è¯¦æƒ…è¯·å‚é˜…é›†æˆé¡µé¢ã€‚å‚æ•°èŠå¤©æ¨¡å‹æ¥å—å¯ç”¨äºé…ç½®å…¶è¡Œä¸ºçš„ä¸€ç»„å‚æ•°ã€‚æ”¯æŒçš„å‚æ•°é›†å› æ¨¡å‹å’Œæä¾›å•†è€Œå¼‚ï¼Œä½†æ ‡å‡†å‚æ•°åŒ…æ‹¬ï¼šå‚æ•°ç±»å‹å¿…å¡«è¯´æ˜modelstringæ˜¯æ‚¨æƒ³ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹çš„åç§°æˆ–æ ‡è¯†ç¬¦ã€‚api_keystringå¦ç”¨äºå‘æ¨¡å‹æä¾›å•†è¿›è¡Œèº«ä»½éªŒè¯çš„å¯†é’¥ã€‚é€šå¸¸åœ¨æ³¨å†Œè®¿é—®æ¨¡å‹æ—¶é¢å‘ã€‚é€šå¸¸é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡è®¿é—®ã€‚temperaturenumberå¦æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ã€‚å€¼è¶Šé«˜ï¼Œå“åº”è¶Šå…·åˆ›é€ æ€§ï¼›å€¼è¶Šä½ï¼Œå“åº”è¶Šç¡®å®šæ€§ã€‚timeoutnumberå¦åœ¨å–æ¶ˆè¯·æ±‚ä¹‹å‰ç­‰å¾…æ¨¡å‹å“åº”çš„æœ€å¤§æ—¶é—´ï¼ˆç§’ï¼‰ã€‚max_tokensnumberå¦é™åˆ¶å“åº”ä¸­çš„ä»¤ç‰Œæ€»æ•°ï¼Œæœ‰æ•ˆæ§åˆ¶è¾“å‡ºé•¿åº¦ã€‚max_retriesnumberå¦å¦‚æœå› ç½‘ç»œè¶…æ—¶æˆ–é€Ÿç‡é™åˆ¶ç­‰é—®é¢˜è€Œå¤±è´¥ï¼Œç³»ç»Ÿå°†é‡æ–°å‘é€è¯·æ±‚çš„æœ€å¤§å°è¯•æ¬¡æ•°ã€‚ä½¿ç”¨ init_chat_modelï¼Œå°†è¿™äº›å‚æ•°ä½œä¸ºå†…è” **kwargs ä¼ é€’ï¼šmodel = init_chat_model(
    "anthropic:claude-sonnet-4-5",
    # ä¼ é€’ç»™æ¨¡å‹çš„ Kwargsï¼š
    temperature=0.7,
    timeout=30,
    max_tokens=1000,
)ä¿¡æ¯ æ¯ä¸ªèŠå¤©æ¨¡å‹é›†æˆå¯èƒ½å…·æœ‰ç”¨äºæ§åˆ¶æä¾›å•†ç‰¹å®šåŠŸèƒ½çš„é¢å¤–å‚æ•°ã€‚ä¾‹å¦‚ï¼ŒChatOpenAI å…·æœ‰ use_responses_api ä»¥å†³å®šæ˜¯å¦ä½¿ç”¨ OpenAI Responses æˆ– Completions APIã€‚ è¦æŸ¥æ‰¾ç»™å®šèŠå¤©æ¨¡å‹æ”¯æŒçš„æ‰€æœ‰å‚æ•°ï¼Œè¯·è½¬åˆ°èŠå¤©æ¨¡å‹é›†æˆé¡µé¢ã€‚è°ƒç”¨å¿…é¡»è°ƒç”¨èŠå¤©æ¨¡å‹ä»¥ç”Ÿæˆè¾“å‡ºã€‚ä¸»è¦æœ‰ä¸‰ç§è°ƒç”¨æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•é€‚ç”¨äºä¸åŒçš„ç”¨ä¾‹ã€‚Invokeè°ƒç”¨æ¨¡å‹çš„æœ€ç›´æ¥æ–¹æ³•æ˜¯ä½¿ç”¨ invoke() ä¼ å…¥å•ä¸ªæ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ")
print(response)å¯ä»¥å‘æ¨¡å‹æä¾›æ¶ˆæ¯åˆ—è¡¨ä»¥è¡¨ç¤ºå¯¹è¯å†å²ã€‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªè§’è‰²ï¼Œæ¨¡å‹ç”¨å®ƒæ¥æŒ‡ç¤ºå¯¹è¯ä¸­è°å‘é€äº†æ¶ˆæ¯ã€‚æœ‰å…³è§’è‰²ã€ç±»å‹å’Œå†…å®¹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¶ˆæ¯æŒ‡å—ã€‚from langchain.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„æœ‰ç”¨åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"},
    {"role": "assistant", "content": "J'adore la programmation."},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚"}
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applications.")from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    SystemMessage("ä½ æ˜¯ä¸€ä¸ªå°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„æœ‰ç”¨åŠ©æ‰‹ã€‚"),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"),
    AIMessage("J'adore la programmationã€‚"),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚")
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applicationsã€‚")Streamå¤§å¤šæ•°æ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆæ—¶æµå¼ä¼ è¾“å…¶è¾“å‡ºå†…å®¹ã€‚é€šè¿‡é€æ­¥æ˜¾ç¤ºè¾“å‡ºï¼Œæµå¼ä¼ è¾“æ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·ä½“éªŒï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒé•¿çš„å“åº”ã€‚è°ƒç”¨ stream() è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œå®ƒåœ¨ç”Ÿæˆæ—¶é€å—äº§ç”Ÿè¾“å‡ºã€‚æ‚¨å¯ä»¥ä½¿ç”¨å¾ªç¯å®æ—¶å¤„ç†æ¯ä¸ªå—ï¼šfor chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    print(chunk.text, end="|", flush=True)for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    for block in chunk.content_blocks:
        if block["type"] == "reasoning" and (reasoning := block.get("reasoning")):
            print(f"æ¨ç†ï¼š{reasoning}")
        elif block["type"] == "tool_call_chunk":
            print(f"å·¥å…·è°ƒç”¨å—ï¼š{block}")
        elif block["type"] == "text":
            print(block["text"])
        else:
            ...ä¸è¿”å›å•ä¸ª AIMessage çš„ invoke() ä¸åŒï¼Œstream() è¿”å›å¤šä¸ª AIMessageChunk å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ä¸€éƒ¨åˆ†è¾“å‡ºæ–‡æœ¬ã€‚é‡è¦çš„æ˜¯ï¼Œæµä¸­çš„æ¯ä¸ªå—éƒ½è®¾è®¡ä¸ºé€šè¿‡æ±‚å’Œèšåˆæˆå®Œæ•´æ¶ˆæ¯ï¼šfull = None  # None | AIMessageChunk
for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    full = chunk if full is None else full + chunk
    print(full.text)

# å¤©ç©º
# å¤©ç©ºæ˜¯
# å¤©ç©ºé€šå¸¸
# å¤©ç©ºé€šå¸¸æ˜¯è“è‰²
# ...

print(full.content_blocks)
# [{"type": "text", "text": "å¤©ç©ºé€šå¸¸æ˜¯è“è‰²..."}]ç”Ÿæˆçš„æ¶ˆæ¯å¯ä»¥ä¸ä½¿ç”¨ invoke() ç”Ÿæˆçš„æ¶ˆæ¯ç›¸åŒå¤„ç†â€”â€”ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥èšåˆæˆæ¶ˆæ¯å†å²å¹¶ä½œä¸ºå¯¹è¯ä¸Šä¸‹æ–‡ä¼ é€’å›æ¨¡å‹ã€‚è­¦å‘Š æµå¼ä¼ è¾“ä»…åœ¨ç¨‹åºçš„æ‰€æœ‰æ­¥éª¤éƒ½çŸ¥é“å¦‚ä½•å¤„ç†å—æµæ—¶æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œæ— æ³•æµå¼ä¼ è¾“çš„åº”ç”¨ç¨‹åºæ˜¯éœ€è¦å…ˆå°†æ•´ä¸ªè¾“å‡ºå­˜å‚¨åœ¨å†…å­˜ä¸­æ‰èƒ½å¤„ç†çš„æƒ…å†µã€‚é«˜çº§æµå¼ä¼ è¾“ä¸»é¢˜â€œè‡ªåŠ¨æµå¼ä¼ è¾“â€èŠå¤©æ¨¡å‹LangChain é€šè¿‡åœ¨æŸäº›æƒ…å†µä¸‹è‡ªåŠ¨å¯ç”¨æµå¼ä¼ è¾“æ¨¡å¼æ¥ç®€åŒ–ä»èŠå¤©æ¨¡å‹è¿›è¡Œæµå¼ä¼ è¾“ï¼Œå³ä½¿æ‚¨æœªæ˜¾å¼è°ƒç”¨æµå¼ä¼ è¾“æ–¹æ³•ã€‚è¿™åœ¨æ‚¨ä½¿ç”¨éæµå¼ä¼ è¾“çš„ invoke æ–¹æ³•ä½†ä»å¸Œæœ›æµå¼ä¼ è¾“æ•´ä¸ªåº”ç”¨ç¨‹åºï¼ˆåŒ…æ‹¬èŠå¤©æ¨¡å‹çš„ä¸­é—´ç»“æœï¼‰æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨ LangGraph ä»£ç† ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨èŠ‚ç‚¹å†…è°ƒç”¨ model.invoke()ï¼Œä½†å¦‚æœåœ¨æµå¼ä¼ è¾“æ¨¡å¼ä¸‹è¿è¡Œï¼ŒLangChain å°†è‡ªåŠ¨å§”æ‰˜ç»™æµå¼ä¼ è¾“ã€‚å·¥ä½œåŸç†å½“æ‚¨ invoke() ä¸€ä¸ªèŠå¤©æ¨¡å‹æ—¶ï¼Œå¦‚æœ LangChain æ£€æµ‹åˆ°æ‚¨æ­£åœ¨å°è¯•æµå¼ä¼ è¾“æ•´ä¸ªåº”ç”¨ç¨‹åºï¼Œå®ƒå°†è‡ªåŠ¨åˆ‡æ¢åˆ°å†…éƒ¨æµå¼ä¼ è¾“æ¨¡å¼ã€‚å¯¹äºä½¿ç”¨ invoke çš„ä»£ç ï¼Œç»“æœæ˜¯ç›¸åŒçš„ï¼›ç„¶è€Œï¼Œåœ¨èŠå¤©æ¨¡å‹æµå¼ä¼ è¾“æ—¶ï¼ŒLangChain å°†è´Ÿè´£åœ¨ LangChain çš„å›è°ƒç³»ç»Ÿä¸­è°ƒç”¨ on_llm_new_token äº‹ä»¶ã€‚å›è°ƒäº‹ä»¶å…è®¸ LangGraph çš„ stream() å’Œ astream_events() å®æ—¶æ˜¾ç¤ºèŠå¤©æ¨¡å‹çš„è¾“å‡ºã€‚æµå¼ä¼ è¾“äº‹ä»¶LangChain èŠå¤©æ¨¡å‹è¿˜å¯ä»¥ä½¿ç”¨ astream_events() æµå¼ä¼ è¾“è¯­ä¹‰äº‹ä»¶ã€‚è¿™ç®€åŒ–äº†åŸºäºäº‹ä»¶ç±»å‹å’Œå…¶ä»–å…ƒæ•°æ®çš„è¿‡æ»¤ï¼Œå¹¶åœ¨åå°èšåˆå®Œæ•´æ¶ˆæ¯ã€‚è¯·å‚é˜…ä»¥ä¸‹ç¤ºä¾‹ã€‚async for event in model.astream_events("ä½ å¥½"):

    if event["event"] == "on_chat_model_start":
        print(f"è¾“å…¥ï¼š{event['data']['input']}")

    elif event["event"] == "on_chat_model_stream":
        print(f"ä»¤ç‰Œï¼š{event['data']['chunk'].text}")

    elif event["event"] == "on_chat_model_end":
        print(f"å®Œæ•´æ¶ˆæ¯ï¼š{event['data']['output'].text}")

    else:
        passè¾“å…¥ï¼šä½ å¥½
ä»¤ç‰Œï¼šä½ 
ä»¤ç‰Œï¼šå¥½
ä»¤ç‰Œï¼šï¼
ä»¤ç‰Œï¼šæˆ‘
ä»¤ç‰Œï¼šèƒ½
ä»¤ç‰Œï¼šå¦‚
...
å®Œæ•´æ¶ˆæ¯ï¼šä½ å¥½ï¼ä»Šå¤©æˆ‘èƒ½å¦‚ä½•å¸®åŠ©æ‚¨ï¼Ÿæç¤º è¯·å‚é˜… astream_events() å‚è€ƒï¼Œäº†è§£äº‹ä»¶ç±»å‹å’Œå…¶ä»–è¯¦ç»†ä¿¡æ¯ã€‚Batchå°†ä¸€ç»„ç‹¬ç«‹è¯·æ±‚æ‰¹é‡å¤„ç†ç»™æ¨¡å‹å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½å¹¶é™ä½æˆæœ¬ï¼Œå› ä¸ºå¤„ç†å¯ä»¥å¹¶è¡Œè¿›è¡Œï¼šresponses = model.batch([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
])
for response in responses:
    print(response)æ³¨æ„ æœ¬èŠ‚æè¿°çš„æ˜¯èŠå¤©æ¨¡å‹æ–¹æ³• batch()ï¼Œå®ƒåœ¨å®¢æˆ·ç«¯å¹¶è¡ŒåŒ–æ¨¡å‹è°ƒç”¨ã€‚ å®ƒä¸åŒäºæ¨ç†æä¾›å•†æ”¯æŒçš„æ‰¹é‡ APIï¼Œä¾‹å¦‚ OpenAI æˆ– Anthropicã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œbatch() ä»…è¿”å›æ•´ä¸ªæ‰¹æ¬¡çš„æœ€ç»ˆè¾“å‡ºã€‚å¦‚æœæ‚¨å¸Œæœ›åœ¨æ¯ä¸ªå•ç‹¬è¾“å…¥å®Œæˆç”Ÿæˆæ—¶æ¥æ”¶è¾“å‡ºï¼Œå¯ä»¥ä½¿ç”¨ batch_as_completed() æµå¼ä¼ è¾“ç»“æœï¼šfor response in model.batch_as_completed([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
]):
    print(response)æ³¨æ„ ä½¿ç”¨ batch_as_completed() æ—¶ï¼Œç»“æœå¯èƒ½æ— åºåˆ°è¾¾ã€‚æ¯ä¸ªç»“æœåŒ…æ‹¬è¾“å…¥ç´¢å¼•ï¼Œä»¥ä¾¿æ ¹æ®éœ€è¦åŒ¹é…å’Œé‡å»ºåŸå§‹é¡ºåºã€‚æç¤º åœ¨ä½¿ç”¨ batch() æˆ– batch_as_completed() å¤„ç†å¤§é‡è¾“å…¥æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ§åˆ¶æœ€å¤§å¹¶è¡Œè°ƒç”¨æ•°ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨ RunnableConfig å­—å…¸ä¸­è®¾ç½® max_concurrency å±æ€§æ¥å®Œæˆã€‚model.batch(
    list_of_inputs,
    config={
        'max_concurrency': 5,  # é™åˆ¶ä¸º 5 ä¸ªå¹¶è¡Œè°ƒç”¨
    }
)æœ‰å…³æ‰¹å¤„ç†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…å‚è€ƒã€‚å·¥å…·è°ƒç”¨æ¨¡å‹å¯ä»¥è¯·æ±‚è°ƒç”¨æ‰§è¡Œä»»åŠ¡çš„å·¥å…·ï¼Œä¾‹å¦‚ä»æ•°æ®åº“è·å–æ•°æ®ã€æœç´¢ç½‘ç»œæˆ–è¿è¡Œä»£ç ã€‚å·¥å…·æ˜¯ä»¥ä¸‹å†…å®¹çš„é…å¯¹ï¼šæ¶æ„ï¼ŒåŒ…æ‹¬å·¥å…·çš„åç§°ã€æè¿°å’Œ/æˆ–å‚æ•°å®šä¹‰ï¼ˆé€šå¸¸æ˜¯ JSON æ¶æ„ï¼‰è¦æ‰§è¡Œçš„å‡½æ•°æˆ–åç¨‹æ³¨æ„ æ‚¨å¯èƒ½ä¼šå¬åˆ°â€œå‡½æ•°è°ƒç”¨â€ä¸€è¯ã€‚æˆ‘ä»¬å°†æ­¤ä¸â€œå·¥å…·è°ƒç”¨â€äº’æ¢ä½¿ç”¨ã€‚è¦ä½¿æ‚¨å®šä¹‰çš„å·¥å…·å¯ä¾›æ¨¡å‹ä½¿ç”¨ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ bind_tools() ç»‘å®šå®ƒä»¬ã€‚åœ¨åç»­è°ƒç”¨ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©è°ƒç”¨ä»»ä½•ç»‘å®šçš„å·¥å…·ã€‚ä¸€äº›æ¨¡å‹æä¾›å•†æä¾›å†…ç½®å·¥å…·ï¼Œå¯é€šè¿‡æ¨¡å‹æˆ–è°ƒç”¨å‚æ•°å¯ç”¨ï¼ˆä¾‹å¦‚ ChatOpenAIã€ChatAnthropicï¼‰ã€‚è¯·æŸ¥çœ‹ç›¸åº”çš„æä¾›å•†å‚è€ƒä»¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚æç¤º æœ‰å…³åˆ›å»ºå·¥å…·çš„è¯¦ç»†ä¿¡æ¯å’Œå…¶ä»–é€‰é¡¹ï¼Œè¯·å‚é˜…å·¥å…·æŒ‡å—ã€‚from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """è·å–æŸä¸ªä½ç½®çš„å¤©æ°”ã€‚"""
    return f"{location} å¤©æ°”æ™´æœ—ã€‚"

model_with_tools = model.bind_tools([get_weather])  # [!code highlight]

response = model_with_tools.invoke("æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
for tool_call in response.tool_calls:
    # æŸ¥çœ‹æ¨¡å‹å‘å‡ºçš„å·¥å…·è°ƒç”¨
    print(f"å·¥å…·ï¼š{tool_call['name']}")
    print(f"å‚æ•°ï¼š{tool_call['args']}")ç»‘å®šç”¨æˆ·å®šä¹‰çš„å·¥å…·æ—¶ï¼Œæ¨¡å‹çš„å“åº”åŒ…æ‹¬è¯·æ±‚æ‰§è¡Œå·¥å…·ã€‚å½“å°†æ¨¡å‹ä¸ä»£ç†åˆ†å¼€ä½¿ç”¨æ—¶ï¼Œæ‚¨éœ€è¦æ‰§è¡Œè¯·æ±‚çš„æ“ä½œå¹¶å°†ç»“æœè¿”å›ç»™æ¨¡å‹ä»¥ç”¨äºåç»­æ¨ç†ã€‚è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨ä»£ç†æ—¶ï¼Œä»£ç†å¾ªç¯å°†ä¸ºæ‚¨å¤„ç†å·¥å…·æ‰§è¡Œå¾ªç¯ã€‚ä¸‹é¢å±•ç¤ºäº†ä¸€äº›ä½¿ç”¨å·¥å…·è°ƒç”¨çš„å¸¸è§æ–¹æ³•ã€‚å·¥å…·æ‰§è¡Œå¾ªç¯å½“æ¨¡å‹è¿”å›å·¥å…·è°ƒç”¨æ—¶ï¼Œæ‚¨éœ€è¦æ‰§è¡Œå·¥å…·å¹¶å°†ç»“æœä¼ é€’å›æ¨¡å‹ã€‚è¿™ä¼šåˆ›å»ºä¸€ä¸ªå¯¹è¯å¾ªç¯ï¼Œæ¨¡å‹å¯ä»¥ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆå…¶æœ€ç»ˆå“åº”ã€‚LangChain åŒ…å«ä»£ç†æŠ½è±¡æ¥ä¸ºæ‚¨å¤„ç†æ­¤åè°ƒã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼š# å°†ï¼ˆå¯èƒ½å¤šä¸ªï¼‰å·¥å…·ç»‘å®šåˆ°æ¨¡å‹
model_with_tools = model.bind_tools([get_weather])

# æ­¥éª¤ 1ï¼šæ¨¡å‹ç”Ÿæˆå·¥å…·è°ƒç”¨
messages = [{"role": "user", "content": "æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# æ­¥éª¤ 2ï¼šæ‰§è¡Œå·¥å…·å¹¶æ”¶é›†ç»“æœ
for tool_call in ai_msg.tool_calls:
    # ä½¿ç”¨ç”Ÿæˆçš„å‚æ•°æ‰§è¡Œå·¥å…·
    tool_result = get_weather.invoke(tool_call)
    messages.append(tool_result)

# æ­¥éª¤ 3ï¼šå°†ç»“æœä¼ é€’å›æ¨¡å‹ä»¥è·å–æœ€ç»ˆå“åº”
final_response = model_with_tools.invoke(messages)
print(final_response.text)
# "æ³¢å£«é¡¿å½“å‰å¤©æ°”ä¸º 72Â°Fï¼Œæ™´æœ—ã€‚"æ¯ä¸ªç”±å·¥å…·è¿”å›çš„ ToolMessage åŒ…å«ä¸€ä¸ªä¸åŸå§‹å·¥å…·è°ƒç”¨åŒ¹é…çš„ tool_call_idï¼Œå¸®åŠ©æ¨¡å‹å°†ç»“æœä¸è¯·æ±‚ç›¸å…³è”ã€‚å¼ºåˆ¶å·¥å…·è°ƒç”¨é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªç”±é€‰æ‹©ä½¿ç”¨å“ªä¸ªç»‘å®šçš„å·¥å…·ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯èƒ½å¸Œæœ›å¼ºåˆ¶é€‰æ‹©å·¥å…·ï¼Œç¡®ä¿æ¨¡å‹ä½¿ç”¨ç‰¹å®šå·¥å…·æˆ–ç»™å®šåˆ—è¡¨ä¸­çš„ä»»ä½•å·¥å…·ï¼šmodel_with_tools = model.bind_tools([tool_1], tool_choice="any")model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")å¹¶è¡Œå·¥å…·è°ƒç”¨è®¸å¤šæ¨¡å‹æ”¯æŒåœ¨é€‚å½“æ—¶å¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…·ã€‚è¿™å…è®¸æ¨¡å‹åŒæ—¶ä»ä¸åŒæ¥æºæ”¶é›†ä¿¡æ¯ã€‚model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke(
    "æ³¢å£«é¡¿å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
)

# æ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆå¤šä¸ªå·¥å…·è°ƒç”¨
print(response.tool_calls)
# [
#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
# ]

# æ‰§è¡Œæ‰€æœ‰å·¥å…·ï¼ˆå¯ä»¥ä½¿ç”¨ async å¹¶è¡Œæ‰§è¡Œï¼‰
results = []
for tool_call in response.tool_calls:
    if tool_call['name'] == 'get_weather':
        result = get_weather.invoke(tool_call)
    ...
    results.append(result)æ¨¡å‹æ ¹æ®è¯·æ±‚æ“ä½œçš„ç‹¬ç«‹æ€§æ™ºèƒ½åœ°ç¡®å®šä½•æ—¶é€‚åˆå¹¶è¡Œæ‰§è¡Œã€‚æç¤º å¤§å¤šæ•°æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹é»˜è®¤å¯ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨ã€‚æŸäº›æ¨¡å‹ï¼ˆåŒ…æ‹¬ OpenAI å’Œ Anthropicï¼‰å…è®¸æ‚¨ç¦ç”¨æ­¤åŠŸèƒ½ã€‚è¦æ‰§è¡Œæ­¤æ“ä½œï¼Œè¯·è®¾ç½® parallel_tool_calls=Falseï¼šmodel.bind_tools([get_weather], parallel_tool_calls=False)æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨åœ¨æµå¼ä¼ è¾“å“åº”æ—¶ï¼Œå·¥å…·è°ƒç”¨é€šè¿‡ ToolCallChunk é€æ­¥æ„å»ºã€‚è¿™å…è®¸æ‚¨åœ¨ç”Ÿæˆå·¥å…·è°ƒç”¨æ—¶æŸ¥çœ‹å®ƒä»¬ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”ã€‚for chunk in model_with_tools.stream(
    "æ³¢å£«é¡¿å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
):
    # å·¥å…·è°ƒç”¨å—é€æ­¥åˆ°è¾¾
    for tool_chunk in chunk.tool_call_chunks:
        if name := tool_chunk.get("name"):
            print(f"å·¥å…·ï¼š{name}")
        if id_ := tool_chunk.get("id"):
            print(f"IDï¼š{id_}")
        if args := tool_chunk.get("args"):
            print(f"å‚æ•°ï¼š{args}")

# è¾“å‡ºï¼š
# å·¥å…·ï¼šget_weather
# IDï¼šcall_SvMlU1TVIZugrFLckFE2ceRE
# å‚æ•°ï¼š{"lo
# å‚æ•°ï¼šcatio
# å‚æ•°ï¼šn": "B
# å‚æ•°ï¼šosto
# å‚æ•°ï¼šn"}
# å·¥å…·ï¼šget_weather
# IDï¼šcall_QMZdy6qInx13oWKE7KhuhOLR
# å‚æ•°ï¼š{"lo
# å‚æ•°ï¼šcatio
# å‚æ•°ï¼šn": "T
# å‚æ•°ï¼šokyo
# å‚æ•°ï¼š"}æ‚¨å¯ä»¥ç´¯ç§¯å—ä»¥æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼šgathered = None
for chunk in model_with_tools.stream("æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"):
    gathered = chunk if gathered is None else gathered + chunk
    print(gathered.tool_calls)ç»“æ„åŒ–è¾“å‡ºå¯ä»¥è¯·æ±‚æ¨¡å‹ä»¥åŒ¹é…ç»™å®šæ¶æ„çš„æ ¼å¼æä¾›å…¶å“åº”ã€‚è¿™å¯¹äºç¡®ä¿è¾“å‡ºæ˜“äºè§£æå¹¶ç”¨äºåç»­å¤„ç†éå¸¸æœ‰ç”¨ã€‚LangChain æ”¯æŒå¤šç§æ¶æ„ç±»å‹å’Œå¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–è¾“å‡ºçš„æ–¹æ³•ã€‚PydanticPydantic æ¨¡å‹ æä¾›æœ€ä¸°å¯Œçš„åŠŸèƒ½é›†ï¼ŒåŒ…æ‹¬å­—æ®µéªŒè¯ã€æè¿°å’ŒåµŒå¥—ç»“æ„ã€‚from pydantic import BaseModel, Field

class Movie(BaseModel):
    """ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: str = Field(..., description="ç”µå½±æ ‡é¢˜")
    year: int = Field(..., description="ç”µå½±ä¸Šæ˜ å¹´ä»½")
    director: str = Field(..., description="ç”µå½±å¯¼æ¼”")
    rating: float = Field(..., description="ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†")

model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)TypedDictTypedDict æä¾›ä½¿ç”¨ Python å†…ç½®ç±»å‹çš„æ›´ç®€å•æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºä¸éœ€è¦è¿è¡Œæ—¶éªŒè¯çš„æƒ…å†µã€‚from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
    """ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: Annotated[str, ..., "ç”µå½±æ ‡é¢˜"]
    year: Annotated[int, ..., "ç”µå½±ä¸Šæ˜ å¹´ä»½"]
    director: Annotated[str, ..., "ç”µå½±å¯¼æ¼”"]
    rating: Annotated[float, ..., "ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†"]

model_with_structure = model.with_structured_output(MovieDict)
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}JSON Schemaä¸ºäº†è·å¾—æœ€å¤§æ§åˆ¶æˆ–äº’æ“ä½œæ€§ï¼Œæ‚¨å¯ä»¥æä¾›åŸå§‹ JSON æ¶æ„ã€‚import json

json_schema = {
    "title": "Movie",
    "description": "ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±",
    "type": "object",
    "properties": {
        "title": {
            "type": "string",
            "description": "ç”µå½±æ ‡é¢˜"
        },
        "year": {
            "type": "integer",
            "description": "ç”µå½±ä¸Šæ˜ å¹´ä»½"
        },
        "director": {
            "type": "string",
            "description": "ç”µå½±å¯¼æ¼”"
        },
        "rating": {
            "type": "number",
            "description": "ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†"
        }
    },
    "required": ["title", "year", "director", "rating"]
}

model_with_structure = model.with_structured_output(
    json_schema,
    method="json_schema",
)
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'Inception', 'year': 2010, ...}æ³¨æ„ç»“æ„åŒ–è¾“å‡ºçš„å…³é”®è€ƒè™‘å› ç´ ï¼šæ–¹æ³•å‚æ•°ï¼šæŸäº›æä¾›å•†æ”¯æŒä¸åŒçš„æ–¹æ³•ï¼ˆ'json_schema'ã€'function_calling'ã€'json_mode'ï¼‰ 'json_schema' é€šå¸¸æŒ‡æä¾›å•†æä¾›çš„ä¸“ç”¨ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½'function_calling' é€šè¿‡å¼ºåˆ¶å·¥å…·è°ƒç”¨éµå¾ªç»™å®šæ¶æ„æ¥æ´¾ç”Ÿç»“æ„åŒ–è¾“å‡º'json_mode' æ˜¯æŸäº›æä¾›å•†æä¾›çš„ 'json_schema' çš„å‰èº«â€”â€”å®ƒç”Ÿæˆæœ‰æ•ˆçš„ JSONï¼Œä½†æ¶æ„å¿…é¡»åœ¨æç¤ºä¸­æè¿°åŒ…å«åŸå§‹ï¼šä½¿ç”¨ include_raw=True ä»¥åŒæ—¶è·å–å·²è§£æçš„è¾“å‡ºå’ŒåŸå§‹ AI æ¶ˆæ¯éªŒè¯ï¼šPydantic æ¨¡å‹æä¾›è‡ªåŠ¨éªŒè¯ï¼Œè€Œ TypedDict å’Œ JSON Schema éœ€è¦æ‰‹åŠ¨éªŒè¯ç¤ºä¾‹ï¼šæ¶ˆæ¯è¾“å‡ºä¸è§£æç»“æ„å¹¶å­˜è¿”å›åŸå§‹ AIMessage å¯¹è±¡ä¸è§£æè¡¨ç¤ºä¸€èµ·ä»¥è®¿é—®å“åº”å…ƒæ•°æ®ï¼ˆå¦‚ä»¤ç‰Œè®¡æ•°ï¼‰å¯èƒ½å¾ˆæœ‰ç”¨ã€‚ä¸ºæ­¤ï¼Œåœ¨è°ƒç”¨ with_structured_output æ—¶è®¾ç½® include_raw=Trueï¼šfrom pydantic import BaseModel, Field

class Movie(BaseModel):
    """ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: str = Field(..., description="ç”µå½±æ ‡é¢˜")
    year: int = Field(..., description="ç”µå½±ä¸Šæ˜ å¹´ä»½")
    director: str = Field(..., description="ç”µå½±å¯¼æ¼”")
    rating: float = Field(..., description="ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†")

model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
response
# {
#     "raw": AIMessage(...),
#     "parsed": Movie(title=..., year=..., ...),
#     "parsing_error": None,
# }ç¤ºä¾‹ï¼šåµŒå¥—ç»“æ„æ¶æ„å¯ä»¥åµŒå¥—ï¼šfrom pydantic import BaseModel, Field

class Actor(BaseModel):
    name: str
    role: str

class MovieDetails(BaseModel):
    title: str
    year: int
    cast: list[Actor]
    genres: list[str]
    budget: float | None = Field(None, description="é¢„ç®—ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰")

model_with_structure = model.with_structured_output(MovieDetails)from typing_extensions import Annotated, TypedDict

class Actor(TypedDict):
    name: str
    role: str

class MovieDetails(TypedDict):
    title: str
    year: int
    cast: list[Actor]
    genres: list[str]
    budget: Annotated[float | None, ..., "é¢„ç®—ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰"]

model_with_structure = model.with_structured_output(MovieDetails)æ”¯æŒçš„æ¨¡å‹LangChain æ”¯æŒæ‰€æœ‰ä¸»è¦æ¨¡å‹æä¾›å•†ï¼ŒåŒ…æ‹¬ OpenAIã€Anthropicã€Googleã€Azureã€AWS Bedrock ç­‰ã€‚æ¯ä¸ªæä¾›å•†æä¾›å…·æœ‰ä¸åŒåŠŸèƒ½çš„å„ç§æ¨¡å‹ã€‚æœ‰å…³ LangChain ä¸­æ”¯æŒçš„æ¨¡å‹å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜…é›†æˆé¡µé¢ã€‚é«˜çº§ä¸»é¢˜å¤šæ¨¡æ€æŸäº›æ¨¡å‹å¯ä»¥å¤„ç†å’Œè¿”å›éæ–‡æœ¬æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚æ‚¨å¯ä»¥é€šè¿‡æä¾›å†…å®¹å—å°†éæ–‡æœ¬æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚æç¤º æ‰€æœ‰å…·æœ‰åº•å±‚å¤šæ¨¡æ€åŠŸèƒ½çš„ LangChain èŠå¤©æ¨¡å‹éƒ½æ”¯æŒï¼šè·¨æä¾›å•†æ ‡å‡†æ ¼å¼çš„æ•°æ®ï¼ˆè¯·å‚é˜…æˆ‘ä»¬çš„æ¶ˆæ¯æŒ‡å—ï¼‰OpenAI èŠå¤©å®Œæˆæ ¼å¼ç‰¹å®šäºè¯¥æä¾›å•†çš„ä»»ä½•æ ¼å¼ï¼ˆä¾‹å¦‚ï¼ŒAnthropic æ¨¡å‹æ¥å— Anthropic åŸç”Ÿæ ¼å¼ï¼‰æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¶ˆæ¯æŒ‡å—çš„å¤šæ¨¡æ€éƒ¨åˆ†ã€‚æç¤º å¹¶éæ‰€æœ‰ LLM éƒ½æ˜¯å¹³ç­‰çš„ï¼ æŸäº›æ¨¡å‹å¯ä»¥ä½œä¸ºå…¶å“åº”çš„ä¸€éƒ¨åˆ†è¿”å›å¤šæ¨¡æ€æ•°æ®ã€‚å¦‚æœè°ƒç”¨å®ƒä»¬è¿™æ ·åšï¼Œåˆ™ç”Ÿæˆçš„ AIMessage å°†å…·æœ‰å¤šæ¨¡æ€ç±»å‹çš„å†…å®¹å—ã€‚response = model.invoke("åˆ›å»ºä¸€å¼ çŒ«çš„å›¾ç‰‡")
print(response.content_blocks)
# [
#     {"type": "text", "text": "è¿™æ˜¯ä¸€å¼ çŒ«çš„å›¾ç‰‡"},
#     {"type": "image", "base64": "...", "mime_type": "image/jpeg"},
# ]æœ‰å…³ç‰¹å®šæä¾›å•†çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…é›†æˆé¡µé¢ã€‚æ¨ç†è¾ƒæ–°çš„æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå¤šæ­¥æ¨ç†ä»¥å¾—å‡ºç»“è®ºã€‚è¿™æ¶‰åŠå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“ç®¡ç†çš„æ­¥éª¤ã€‚å¦‚æœåº•å±‚æ¨¡å‹æ”¯æŒï¼Œæ‚¨å¯ä»¥æ˜¾ç¤ºæ­¤æ¨ç†è¿‡ç¨‹ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹å¦‚ä½•å¾—å‡ºå…¶æœ€ç»ˆç­”æ¡ˆã€‚for chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
    print(reasoning_steps if reasoning_steps else chunk.text)response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ")
reasoning_steps = [b for b in response.content_blocks if b["type"] == "reasoning"]
print(" ".join(step["reasoning"] for step in reasoning_steps))æ ¹æ®æ¨¡å‹ï¼Œæ‚¨æœ‰æ—¶å¯ä»¥æŒ‡å®šå®ƒåº”æŠ•å…¥æ¨ç†çš„åŠªåŠ›ç¨‹åº¦ã€‚åŒæ ·ï¼Œæ‚¨å¯ä»¥è¯·æ±‚æ¨¡å‹å®Œå…¨å…³é—­æ¨ç†ã€‚è¿™å¯èƒ½é‡‡ç”¨â€œå±‚çº§â€ï¼ˆä¾‹å¦‚ 'low' æˆ– 'high'ï¼‰æˆ–æ•´æ•°ä»¤ç‰Œé¢„ç®—çš„å½¢å¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…é›†æˆé¡µé¢æˆ–æ‚¨ç›¸åº”èŠå¤©æ¨¡å‹çš„å‚è€ƒã€‚æœ¬åœ°æ¨¡å‹LangChain æ”¯æŒåœ¨æ‚¨è‡ªå·±çš„ç¡¬ä»¶ä¸Šæœ¬åœ°è¿è¡Œæ¨¡å‹ã€‚è¿™å¯¹äºæ•°æ®éšç§è‡³å…³é‡è¦ã€æ‚¨æƒ³è°ƒç”¨è‡ªå®šä¹‰æ¨¡å‹æˆ–å¸Œæœ›é¿å…ä½¿ç”¨åŸºäºäº‘çš„æ¨¡å‹æ‰€äº§ç”Ÿçš„æˆæœ¬çš„åœºæ™¯éå¸¸æœ‰ç”¨ã€‚Ollama æ˜¯æœ¬åœ°è¿è¡Œæ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•ä¹‹ä¸€ã€‚è¯·å‚é˜…é›†æˆé¡µé¢ä¸Šçš„æœ¬åœ°é›†æˆå®Œæ•´åˆ—è¡¨ã€‚æç¤ºç¼“å­˜è®¸å¤šæä¾›å•†æä¾›æç¤ºç¼“å­˜åŠŸèƒ½ï¼Œä»¥å‡å°‘å¯¹ç›¸åŒä»¤ç‰Œé‡å¤å¤„ç†çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚è¿™äº›åŠŸèƒ½å¯ä»¥æ˜¯éšå¼æˆ–æ˜¾å¼ï¼šéšå¼æç¤ºç¼“å­˜ï¼š å¦‚æœè¯·æ±‚å‘½ä¸­ç¼“å­˜ï¼Œæä¾›å•†å°†è‡ªåŠ¨ä¼ é€’æˆæœ¬èŠ‚çœã€‚ç¤ºä¾‹ï¼šOpenAI å’Œ Geminiï¼ˆGemini 2.5 åŠä»¥ä¸Šï¼‰ã€‚æ˜¾å¼ç¼“å­˜ï¼š æä¾›å•†å…è®¸æ‚¨æ‰‹åŠ¨æŒ‡ç¤ºç¼“å­˜ç‚¹ä»¥è·å¾—æ›´å¤§æ§åˆ¶æˆ–ä¿è¯æˆæœ¬èŠ‚çœã€‚ç¤ºä¾‹ï¼šChatOpenAIï¼ˆé€šè¿‡ prompt_cache_keyï¼‰ã€Anthropic çš„ AnthropicPromptCachingMiddleware å’Œ cache_control é€‰é¡¹ã€AWS Bedrockã€Geminiã€‚è­¦å‘Š æç¤ºç¼“å­˜é€šå¸¸ä»…åœ¨è¶…è¿‡æœ€å°è¾“å…¥ä»¤ç‰Œé˜ˆå€¼æ—¶æ‰ä¼šå¯ç”¨ã€‚è¯·å‚é˜…æä¾›å•†é¡µé¢ä»¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚ç¼“å­˜ä½¿ç”¨æƒ…å†µå°†åæ˜ åœ¨æ¨¡å‹å“åº”çš„ä½¿ç”¨å…ƒæ•°æ®ä¸­ã€‚æœåŠ¡å™¨ç«¯å·¥å…·ä½¿ç”¨æŸäº›æä¾›å•†æ”¯æŒæœåŠ¡å™¨ç«¯å·¥å…·è°ƒç”¨å¾ªç¯ï¼šæ¨¡å‹å¯ä»¥åœ¨å•ä¸ªå¯¹è¯è½®æ¬¡ä¸­ä¸ç½‘ç»œæœç´¢ã€ä»£ç è§£é‡Šå™¨å’Œå…¶ä»–å·¥å…·äº¤äº’å¹¶åˆ†æç»“æœã€‚å¦‚æœæ¨¡å‹åœ¨æœåŠ¡å™¨ç«¯è°ƒç”¨å·¥å…·ï¼Œåˆ™å“åº”æ¶ˆæ¯çš„å†…å®¹å°†åŒ…æ‹¬è¡¨ç¤ºå·¥å…·è°ƒç”¨å’Œç»“æœçš„å†…å®¹ã€‚è®¿é—®å“åº”çš„å†…å®¹å—å°†ä»¥æä¾›å•†æ— å…³çš„æ ¼å¼è¿”å›æœåŠ¡å™¨ç«¯å·¥å…·è°ƒç”¨å’Œç»“æœï¼šfrom langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("ä»Šå¤©æœ‰ä»€ä¹ˆæ­£é¢æ–°é—»ï¼Ÿ")
response.content_blocks[
    {
        "type": "server_tool_call",
        "name": "web_search",
        "args": {
            "query": "positive news stories today",
            "type": "search"
        },
        "id": "ws_abc123"
    },
    {
        "type": "server_tool_result",
        "tool_call_id": "ws_abc123",
        "status": "success"
    },
    {
        "type": "text",
        "text": "ä»¥ä¸‹æ˜¯ä»Šå¤©çš„ä¸€äº›æ­£é¢æ–°é—»...",
        "annotations": [
            {
                "end_index": 410,
                "start_index": 337,
                "title": "æ–‡ç« æ ‡é¢˜",
                "type": "citation",
                "url": "..."
            }
        ]
    }
]è¿™è¡¨ç¤ºå•ä¸ªå¯¹è¯è½®æ¬¡ï¼›æ²¡æœ‰éœ€è¦ä½œä¸ºå®¢æˆ·ç«¯å·¥å…·è°ƒç”¨ä¼ å…¥çš„å…³è” ToolMessage å¯¹è±¡ã€‚æœ‰å…³å¯ç”¨å·¥å…·å’Œä½¿ç”¨è¯¦ç»†ä¿¡æ¯çš„ç»™å®šæä¾›å•†ï¼Œè¯·å‚é˜…é›†æˆé¡µé¢ã€‚é€Ÿç‡é™åˆ¶è®¸å¤šèŠå¤©æ¨¡å‹æä¾›å•†å¯¹ç»™å®šæ—¶é—´æ®µå†…å¯ä»¥å‘å‡ºçš„è°ƒç”¨æ¬¡æ•°æ–½åŠ é™åˆ¶ã€‚å¦‚æœæ‚¨è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œé€šå¸¸ä¼šä»æä¾›å•†æ”¶åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯å“åº”ï¼Œå¹¶ä¸”éœ€è¦ç­‰å¾…æ‰èƒ½å‘å‡ºæ›´å¤šè¯·æ±‚ã€‚ä¸ºäº†å¸®åŠ©ç®¡ç†é€Ÿç‡é™åˆ¶ï¼ŒèŠå¤©æ¨¡å‹é›†æˆåœ¨åˆå§‹åŒ–æœŸé—´æ¥å— rate_limiter å‚æ•°ï¼Œä»¥æ§åˆ¶å‘å‡ºè¯·æ±‚çš„é€Ÿç‡ã€‚åˆå§‹åŒ–å’Œä½¿ç”¨é€Ÿç‡é™åˆ¶å™¨LangChain å†…ç½®ï¼ˆå¯é€‰ï¼‰InMemoryRateLimiterã€‚æ­¤é™åˆ¶å™¨æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥ç”±åŒä¸€è¿›ç¨‹ä¸­çš„å¤šä¸ªçº¿ç¨‹å…±äº«ã€‚from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.1,  # æ¯ 10 ç§’ 1 ä¸ªè¯·æ±‚
    check_every_n_seconds=0.1,  # æ¯ 100 æ¯«ç§’æ£€æŸ¥æ˜¯å¦å…è®¸å‘å‡ºè¯·æ±‚
    max_bucket_size=10,  # æ§åˆ¶æœ€å¤§çªå‘å¤§å°ã€‚
)

model = init_chat_model(
    model="gpt-5",
    model_provider="openai",
    rate_limiter=rate_limiter  # [!code highlight]
)è­¦å‘Š æä¾›çš„é€Ÿç‡é™åˆ¶å™¨åªèƒ½é™åˆ¶æ¯å•ä½æ—¶é—´çš„è¯·æ±‚æ•°ã€‚å¦‚æœæ‚¨è¿˜éœ€è¦æ ¹æ®è¯·æ±‚å¤§å°è¿›è¡Œé™åˆ¶ï¼Œå®ƒå°†æ— æµäºäº‹ã€‚åŸºç¡€ URL æˆ–ä»£ç†å¯¹äºè®¸å¤šèŠå¤©æ¨¡å‹é›†æˆï¼Œæ‚¨å¯ä»¥é…ç½® API è¯·æ±‚çš„åŸºç¡€ URLï¼Œè¿™å…è®¸æ‚¨ä½¿ç”¨å…·æœ‰ OpenAI å…¼å®¹ API çš„æ¨¡å‹æä¾›å•†æˆ–ä½¿ç”¨ä»£ç†æœåŠ¡å™¨ã€‚åŸºç¡€ URLè®¸å¤šæ¨¡å‹æä¾›å•†æä¾› OpenAI å…¼å®¹çš„ APIï¼ˆä¾‹å¦‚ï¼ŒTogether AIã€vLLMï¼‰ã€‚æ‚¨å¯ä»¥é€šè¿‡æŒ‡å®šé€‚å½“çš„ base_url å‚æ•°ä½¿ç”¨ init_chat_model ä¸è¿™äº›æä¾›å•†ä¸€èµ·ä½¿ç”¨ï¼šmodel = init_chat_model(
    model="MODEL_NAME",
    model_provider="openai",
    base_url="BASE_URL",
    api_key="YOUR_API_KEY",
)æ³¨æ„ ä½¿ç”¨ç›´æ¥èŠå¤©æ¨¡å‹ç±»å®ä¾‹åŒ–æ—¶ï¼Œå‚æ•°åç§°å¯èƒ½å› æä¾›å•†è€Œå¼‚ã€‚è¯·æŸ¥çœ‹ç›¸åº”çš„å‚è€ƒä»¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚ä»£ç†é…ç½®å¯¹äºéœ€è¦ HTTP ä»£ç†çš„éƒ¨ç½²ï¼ŒæŸäº›æ¨¡å‹é›†æˆæ”¯æŒä»£ç†é…ç½®ï¼šfrom langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    openai_proxy="http://proxy.example.com:8080"
)æ³¨æ„ ä»£ç†æ”¯æŒå› é›†æˆè€Œå¼‚ã€‚è¯·æŸ¥çœ‹ç‰¹å®šæ¨¡å‹æä¾›å•†çš„å‚è€ƒä»¥äº†è§£ä»£ç†é…ç½®é€‰é¡¹ã€‚å¯¹æ•°æ¦‚ç‡æŸäº›æ¨¡å‹å¯ä»¥é€šè¿‡åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶è®¾ç½® logprobs å‚æ•°æ¥é…ç½®ä¸ºè¿”å›è¡¨ç¤ºç»™å®šä»¤ç‰Œå¯èƒ½æ€§çš„ä»¤ç‰Œçº§å¯¹æ•°æ¦‚ç‡ï¼šmodel = init_chat_model(
    model="gpt-4o",
    model_provider="openai"
).bind(logprobs=True)

response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰ä¼šè¯´è¯ï¼Ÿ")
print(response.response_metadata["logprobs"])ä»¤ç‰Œä½¿ç”¨æƒ…å†µè®¸å¤šæ¨¡å‹æä¾›å•†å°†ä»¤ç‰Œä½¿ç”¨ä¿¡æ¯ä½œä¸ºè°ƒç”¨å“åº”çš„ä¸€éƒ¨åˆ†è¿”å›ã€‚å½“å¯ç”¨æ—¶ï¼Œæ­¤ä¿¡æ¯å°†åŒ…å«åœ¨ç›¸åº”æ¨¡å‹ç”Ÿæˆçš„ AIMessage å¯¹è±¡ä¸Šã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¶ˆæ¯æŒ‡å—ã€‚æ³¨æ„ æŸäº›æä¾›å•† APIï¼Œç‰¹åˆ«æ˜¯åœ¨æµå¼ä¼ è¾“ä¸Šä¸‹æ–‡ä¸­ OpenAI å’Œ Azure OpenAI èŠå¤©å®Œæˆï¼Œè¦æ±‚ç”¨æˆ·é€‰æ‹©æ¥æ”¶ä»¤ç‰Œä½¿ç”¨æ•°æ®ã€‚è¯·å‚é˜…é›†æˆæŒ‡å—çš„æµå¼ä¼ è¾“ä½¿ç”¨å…ƒæ•°æ®éƒ¨åˆ†ä»¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å›è°ƒæˆ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·Ÿè¸ªåº”ç”¨ç¨‹åºä¸­è·¨æ¨¡å‹çš„èšåˆä»¤ç‰Œè®¡æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šå›è°ƒå¤„ç†ç¨‹åºfrom langchain.chat_models import init_chat_model
from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="openai:gpt-4o-mini")
model_2 = init_chat_model(model="anthropic:claude-3-5-haiku-latest")

callback = UsageMetadataCallbackHandler()
result_1 = model_1.invoke("ä½ å¥½", config={"callbacks": [callback]})
result_2 = model_2.invoke("ä½ å¥½", config={"callbacks": [callback]})
callback.usage_metadata{
    'gpt-4o-mini-2024-07-18': {
        'input_tokens': 8,
        'output_tokens': 10,
        'total_tokens': 18,
        'input_token_details': {'audio': 0, 'cache_read': 0},
        'output_token_details': {'audio': 0, 'reasoning': 0}
    },
    'claude-3-5-haiku-20241022': {
        'input_tokens': 8,
        'output_tokens': 21,
        'total_tokens': 29,
        'input_token_details': {'cache_read': 0, 'cache_creation': 0}
    }
}ä¸Šä¸‹æ–‡ç®¡ç†å™¨from langchain.chat_models import init_chat_model
from langchain_core.callbacks import get_usage_metadata_callback

model_1 = init_chat_model(model="openai:gpt-4o-mini")
model_2 = init_chat_model(model="anthropic:claude-3-5-haiku-latest")

with get_usage_metadata_callback() as cb:
    model_1.invoke("ä½ å¥½")
    model_2.invoke("ä½ å¥½")
    print(cb.usage_metadata){
    'gpt-4o-mini-2024-07-18': {
        'input_tokens': 8,
        'output_tokens': 10,
        'total_tokens': 18,
        'input_token_details': {'audio': 0, 'cache_read': 0},
        'output_token_details': {'audio': 0, 'reasoning': 0}
    },
    'claude-3-5-haiku-20241022': {
        'input_tokens': 8,
        'output_tokens': 21,
        'total_tokens': 29,
        'input_token_details': {'cache_read': 0, 'cache_creation': 0}
    }
}è°ƒç”¨é…ç½®è°ƒç”¨æ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ RunnableConfig å­—å…¸é€šè¿‡ config å‚æ•°ä¼ é€’é¢å¤–é…ç½®ã€‚è¿™æä¾›äº†å¯¹æ‰§è¡Œè¡Œä¸ºã€å›è°ƒå’Œå…ƒæ•°æ®è·Ÿè¸ªçš„è¿è¡Œæ—¶æ§åˆ¶ã€‚å¸¸è§é…ç½®é€‰é¡¹åŒ…æ‹¬ï¼šresponse = model.invoke(
    "è®²ä¸ªç¬‘è¯",
    config={
        "run_name": "joke_generation",      # æ­¤è¿è¡Œçš„è‡ªå®šä¹‰åç§°
        "tags": ["humor", "demo"],          # ç”¨äºåˆ†ç±»çš„æ ‡ç­¾
        "metadata": {"user_id": "123"},     # è‡ªå®šä¹‰å…ƒæ•°æ®
        "callbacks": [my_callback_handler], # å›è°ƒå¤„ç†ç¨‹åº
    }
)è¿™äº›é…ç½®å€¼åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ç”¨ï¼šä½¿ç”¨ LangSmith è·Ÿè¸ªè¿›è¡Œè°ƒè¯•å®ç°è‡ªå®šä¹‰æ—¥å¿—æˆ–ç›‘æ§åœ¨ç”Ÿäº§ä¸­æ§åˆ¶èµ„æºä½¿ç”¨åœ¨å¤æ‚ç®¡é“ä¸­è·Ÿè¸ªè°ƒç”¨å…³é”®é…ç½®å±æ€§å±æ€§ç±»å‹è¯´æ˜run_namestringåœ¨æ—¥å¿—å’Œè·Ÿè¸ªä¸­æ ‡è¯†æ­¤ç‰¹å®šè°ƒç”¨ã€‚ä¸ç”±å­è°ƒç”¨ç»§æ‰¿ã€‚tagsstring[]ç”±æ‰€æœ‰å­è°ƒç”¨ç»§æ‰¿çš„æ ‡ç­¾ï¼Œç”¨äºåœ¨è°ƒè¯•å·¥å…·ä¸­è¿‡æ»¤å’Œç»„ç»‡ã€‚metadataobjectç”¨äºè·Ÿè¸ªé¢å¤–ä¸Šä¸‹æ–‡çš„è‡ªå®šä¹‰é”®å€¼å¯¹ï¼Œç”±æ‰€æœ‰å­è°ƒç”¨ç»§æ‰¿ã€‚max_concurrencynumberåœ¨ä½¿ç”¨ batch() æˆ– batch_as_completed() æ—¶æ§åˆ¶æœ€å¤§å¹¶è¡Œè°ƒç”¨æ•°ã€‚callbacksarrayç”¨äºåœ¨æ‰§è¡ŒæœŸé—´ç›‘æ§å’Œå“åº”äº‹ä»¶çš„å¤„ç†ç¨‹åºã€‚recursion_limitnumberé“¾çš„æœ€å¤§é€’å½’æ·±åº¦ï¼Œä»¥é˜²æ­¢å¤æ‚ç®¡é“ä¸­çš„æ— é™å¾ªç¯ã€‚æç¤º è¯·å‚é˜…å®Œæ•´çš„ RunnableConfig å‚è€ƒï¼Œäº†è§£æ‰€æœ‰æ”¯æŒçš„å±æ€§ã€‚å¯é…ç½®æ¨¡å‹æ‚¨è¿˜å¯ä»¥é€šè¿‡æŒ‡å®š configurable_fields åˆ›å»ºè¿è¡Œæ—¶å¯é…ç½®æ¨¡å‹ã€‚å¦‚æœæ‚¨æœªæŒ‡å®šæ¨¡å‹å€¼ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹ 'model' å’Œ 'model_provider' å°†æ˜¯å¯é…ç½®çš„ã€‚from langchain.chat_models import init_chat_model

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "ä½ å«ä»€ä¹ˆåå­—",
    config={"configurable": {"model": "gpt-5-nano"}},  # ä½¿ç”¨ GPT-5-Nano è¿è¡Œ
)
configurable_model.invoke(
    "ä½ å«ä»€ä¹ˆåå­—",
    config={"configurable": {"model": "claude-sonnet-4-5"}},  # ä½¿ç”¨ Claude è¿è¡Œ
)å…·æœ‰é»˜è®¤å€¼çš„å¯é…ç½®æ¨¡å‹æˆ‘ä»¬å¯ä»¥åˆ›å»ºå…·æœ‰é»˜è®¤æ¨¡å‹å€¼çš„å¯é…ç½®æ¨¡å‹ï¼ŒæŒ‡å®šå“ªäº›å‚æ•°æ˜¯å¯é…ç½®çš„ï¼Œå¹¶ä¸ºå¯é…ç½®å‚æ•°æ·»åŠ å‰ç¼€ï¼šfirst_model = init_chat_model(
        model="gpt-4.1-mini",
        temperature=0,
        configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
        config_prefix="first",  # å½“é“¾ä¸­æœ‰å¤šä¸ªæ¨¡å‹æ—¶å¾ˆæœ‰ç”¨
)

first_model.invoke("ä½ å«ä»€ä¹ˆåå­—")first_model.invoke(
    "ä½ å«ä»€ä¹ˆåå­—",
    config={
        "configurable": {
            "first_model": "claude-sonnet-4-5",
            "first_temperature": 0.5,
            "first_max_tokens": 100,
        }
    },
)ä»¥å£°æ˜æ–¹å¼ä½¿ç”¨å¯é…ç½®æ¨¡å‹æˆ‘ä»¬å¯ä»¥åœ¨å¯é…ç½®æ¨¡å‹ä¸Šè°ƒç”¨å£°æ˜æ€§æ“ä½œï¼Œå¦‚ bind_toolsã€with_structured_outputã€with_configurable ç­‰ï¼Œå¹¶ä»¥ä¸å¸¸è§„å®ä¾‹åŒ–èŠå¤©æ¨¡å‹å¯¹è±¡ç›¸åŒçš„æ–¹å¼é“¾æ¥å¯é…ç½®æ¨¡å‹ã€‚from pydantic import BaseModel, Field

class GetWeather(BaseModel):
    """è·å–ç»™å®šä½ç½®çš„å½“å‰å¤©æ°”"""
    location: str = Field(..., description="åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚ San Francisco, CA")

class GetPopulation(BaseModel):
    """è·å–ç»™å®šä½ç½®çš„å½“å‰äººå£"""
    location: str = Field(..., description="åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚ San Francisco, CA")

model = init_chat_model(temperature=0)
model_with_tools = model.bind_tools([GetWeather, GetPopulation])

model_with_tools.invoke(
    "2024 å¹´æ´›æ‰çŸ¶å’Œçº½çº¦å“ªä¸ªæ›´å¤§", config={"configurable": {"model": "gpt-4.1-mini"}}
).tool_calls[
    {
        'name': 'GetPopulation',
        'args': {'location': 'Los Angeles, CA'},
        'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
        'type': 'tool_call'
    },
    {
        'name': 'GetPopulation',
        'args': {'location': 'New York, NY'},
        'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
        'type': 'tool_call'
    }
]model_with_tools.invoke(
    "2024 å¹´æ´›æ‰çŸ¶å’Œçº½çº¦å“ªä¸ªæ›´å¤§",
        config={"configurable": {"model": "claude-sonnet-4-5"}},
).tool_calls[
    {
        'name': 'GetPopulation',
        'args': {'location': 'Los Angeles, CA'},
        'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
        'type': 'tool_call'
    },
    {
        'name': 'GetPopulation',
        'args': {'location': 'New York City, NY'},
        'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
        'type': 'tool_call'
    }
]æœ€è¿‘æ›´æ–°2025/10/25 19:51ä¸Šä¸€é¡µæ™ºèƒ½ä½“ä¸‹ä¸€é¡µæ¶ˆæ¯

---

## æ™ºèƒ½ä½“

**URL:** https://langchain-doc.cn/v1/python/langchain/agents.html

æ™ºèƒ½ä½“æ™ºèƒ½ä½“å°†è¯­è¨€æ¨¡å‹ä¸å·¥å…·ç»“åˆï¼Œåˆ›å»ºèƒ½å¤Ÿå¯¹ä»»åŠ¡è¿›è¡Œæ¨ç†ã€å†³å®šä½¿ç”¨å“ªäº›å·¥å…·å¹¶è¿­ä»£å¯»æ±‚è§£å†³æ–¹æ¡ˆçš„ç³»ç»Ÿã€‚create_agent æä¾›äº†ä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„æ™ºèƒ½ä½“å®ç°ã€‚LLM æ™ºèƒ½ä½“åœ¨å¾ªç¯ä¸­è¿è¡Œå·¥å…·ä»¥å®ç°ç›®æ ‡ã€‚ æ™ºèƒ½ä½“ä¼šä¸€ç›´è¿è¡Œï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶â€”â€”å³æ¨¡å‹è¾“å‡ºæœ€ç»ˆç»“æœæˆ–è¾¾åˆ°è¿­ä»£æ¬¡æ•°é™åˆ¶ã€‚ä¿¡æ¯create_agent ä½¿ç”¨ LangGraph æ„å»ºäº†ä¸€ä¸ªåŸºäºå›¾çš„æ™ºèƒ½ä½“è¿è¡Œæ—¶ã€‚å›¾ç”±èŠ‚ç‚¹ï¼ˆæ­¥éª¤ï¼‰å’Œè¾¹ï¼ˆè¿æ¥ï¼‰ç»„æˆï¼Œå®šä¹‰äº†æ™ºèƒ½ä½“å¦‚ä½•å¤„ç†ä¿¡æ¯ã€‚æ™ºèƒ½ä½“åœ¨å›¾ä¸­ç§»åŠ¨ï¼Œæ‰§è¡ŒèŠ‚ç‚¹ï¼Œå¦‚æ¨¡å‹èŠ‚ç‚¹ï¼ˆè°ƒç”¨æ¨¡å‹ï¼‰ã€å·¥å…·èŠ‚ç‚¹ï¼ˆæ‰§è¡Œå·¥å…·ï¼‰æˆ–ä¸­é—´ä»¶ã€‚äº†è§£æ›´å¤šå…³äº Graph API çš„ä¿¡æ¯ã€‚æ ¸å¿ƒç»„ä»¶æ¨¡å‹æ¨¡å‹ æ˜¯æ™ºèƒ½ä½“çš„æ¨ç†å¼•æ“ã€‚å®ƒå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼æŒ‡å®šï¼Œæ”¯æŒé™æ€å’ŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ã€‚é™æ€æ¨¡å‹é™æ€æ¨¡å‹åœ¨åˆ›å»ºæ™ºèƒ½ä½“æ—¶é…ç½®ä¸€æ¬¡ï¼Œå¹¶åœ¨æ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ã€‚è¿™æ˜¯æœ€å¸¸è§ä¸”ç›´æ¥çš„æ–¹æ³•ã€‚ä»  åˆå§‹åŒ–é™æ€æ¨¡å‹ï¼šfrom langchain.agents import create_agent

agent = create_agent(
    "openai:gpt-5",
    tools=tools
)æç¤º æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²æ”¯æŒè‡ªåŠ¨æ¨æ–­ï¼ˆä¾‹å¦‚ "gpt-5" å°†è¢«æ¨æ–­ä¸º "openai:gpt-5"ï¼‰ã€‚è¯·å‚è€ƒå‚è€ƒæ–‡æ¡£ æŸ¥çœ‹å®Œæ•´çš„æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²æ˜ å°„åˆ—è¡¨ã€‚ä¸ºäº†å¯¹æ¨¡å‹é…ç½®è¿›è¡Œæ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æä¾›å•†åŒ…åˆå§‹åŒ–æ¨¡å‹å®ä¾‹ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ ChatOpenAIã€‚è¯·å‚é˜… èŠå¤©æ¨¡å‹ ä»¥äº†è§£å…¶ä»–å¯ç”¨çš„èŠå¤©æ¨¡å‹ç±»ã€‚from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-5",
    temperature=0.1,
    max_tokens=1000,
    timeout=30
    # ...ï¼ˆå…¶ä»–å‚æ•°ï¼‰
)
agent = create_agent(model, tools=tools)æ¨¡å‹å®ä¾‹ä¸ºæ‚¨æä¾›å¯¹é…ç½®çš„å®Œå…¨æ§åˆ¶ã€‚å½“æ‚¨éœ€è¦è®¾ç½®ç‰¹å®šå‚æ•°ï¼Œå¦‚ temperatureã€max_tokensã€timeoutsã€base_url å’Œå…¶ä»–æä¾›å•†ç‰¹å®šè®¾ç½®æ—¶ï¼Œè¯·ä½¿ç”¨å®ƒä»¬ã€‚è¯·å‚è€ƒå‚è€ƒæ–‡æ¡£ æŸ¥çœ‹æ¨¡å‹ä¸Šå¯ç”¨çš„å‚æ•°å’Œæ–¹æ³•ã€‚åŠ¨æ€æ¨¡å‹åŠ¨æ€æ¨¡å‹åœ¨  æ ¹æ®å½“å‰  å’Œä¸Šä¸‹æ–‡è¿›è¡Œé€‰æ‹©ã€‚è¿™æ”¯æŒå¤æ‚çš„è·¯ç”±é€»è¾‘å’Œæˆæœ¬ä¼˜åŒ–ã€‚è¦ä½¿ç”¨åŠ¨æ€æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ @wrap_model_call è£…é¥°å™¨åˆ›å»ºä¸­é—´ä»¶ï¼Œä»¥ä¿®æ”¹è¯·æ±‚ä¸­çš„æ¨¡å‹ï¼šfrom langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse


basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:
    """æ ¹æ®å¯¹è¯å¤æ‚æ€§é€‰æ‹©æ¨¡å‹ã€‚"""
    message_count = len(request.state["messages"])

    if message_count > 10:
        # å¯¹è¾ƒé•¿çš„å¯¹è¯ä½¿ç”¨é«˜çº§æ¨¡å‹
        model = advanced_model
    else:
        model = basic_model

    request.model = model
    return handler(request)

agent = create_agent(
    model=basic_model,  # é»˜è®¤æ¨¡å‹
    tools=tools,
    middleware=[dynamic_model_selection]
)è­¦å‘Š åœ¨ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºæ—¶ï¼Œä¸æ”¯æŒé¢„ç»‘å®šæ¨¡å‹ï¼ˆå·²è°ƒç”¨ bind_tools çš„æ¨¡å‹ï¼‰ã€‚å¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºçš„åŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼Œè¯·ç¡®ä¿ä¼ é€’ç»™ä¸­é—´ä»¶çš„æ¨¡å‹æœªé¢„ç»‘å®šã€‚æç¤º æœ‰å…³æ¨¡å‹é…ç½®è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… æ¨¡å‹ã€‚æœ‰å…³åŠ¨æ€æ¨¡å‹é€‰æ‹©æ¨¡å¼ï¼Œè¯·å‚é˜… ä¸­é—´ä»¶ä¸­çš„åŠ¨æ€æ¨¡å‹ã€‚å·¥å…·å·¥å…·èµ‹äºˆæ™ºèƒ½ä½“æ‰§è¡Œè¡ŒåŠ¨çš„èƒ½åŠ›ã€‚æ™ºèƒ½ä½“è¶…è¶Šäº†ç®€å•çš„ä»…æ¨¡å‹å·¥å…·ç»‘å®šï¼Œå®ç°äº†ï¼šåºåˆ—ä¸­çš„å¤šä¸ªå·¥å…·è°ƒç”¨ï¼ˆç”±å•ä¸ªæç¤ºè§¦å‘ï¼‰é€‚å½“çš„å¹¶è¡Œå·¥å…·è°ƒç”¨åŸºäºå…ˆå‰ç»“æœçš„åŠ¨æ€å·¥å…·é€‰æ‹©å·¥å…·é‡è¯•é€»è¾‘å’Œé”™è¯¯å¤„ç†å·¥å…·è°ƒç”¨ä¹‹é—´çš„çŠ¶æ€æŒä¹…åŒ–æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… å·¥å…·ã€‚å®šä¹‰å·¥å…·å°†å·¥å…·åˆ—è¡¨ä¼ é€’ç»™æ™ºèƒ½ä½“ã€‚from langchain.tools import tool
from langchain.agents import create_agent


@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯ã€‚"""
    return f"ç»“æœï¼š{query}"

@tool
def get_weather(location: str) -> str:
    """è·å–ä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚"""
    return f"{location} çš„å¤©æ°”ï¼šæ™´æœ—ï¼Œ72Â°F"

agent = create_agent(model, tools=[search, get_weather])å¦‚æœæä¾›ç©ºå·¥å…·åˆ—è¡¨ï¼Œæ™ºèƒ½ä½“å°†ä»…åŒ…å«ä¸€ä¸ª LLM èŠ‚ç‚¹ï¼Œä¸å…·å¤‡å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚å·¥å…·é”™è¯¯å¤„ç†è¦è‡ªå®šä¹‰å·¥å…·é”™è¯¯çš„å¤„ç†æ–¹å¼ï¼Œè¯·ä½¿ç”¨ @wrap_tool_call è£…é¥°å™¨åˆ›å»ºä¸­é—´ä»¶ï¼šfrom langchain.agents import create_agent
from langchain.agents.middleware import wrap_tool_call
from langchain_core.messages import ToolMessage


@wrap_tool_call
def handle_tool_errors(request, handler):
    """ä½¿ç”¨è‡ªå®šä¹‰æ¶ˆæ¯å¤„ç†å·¥å…·æ‰§è¡Œé”™è¯¯ã€‚"""
    try:
        return handler(request)
    except Exception as e:
        # å‘æ¨¡å‹è¿”å›è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯
        return ToolMessage(
            content=f"å·¥å…·é”™è¯¯ï¼šè¯·æ£€æŸ¥æ‚¨çš„è¾“å…¥å¹¶é‡è¯•ã€‚({str(e)})",
            tool_call_id=request.tool_call["id"]
        )

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search, get_weather],
    middleware=[handle_tool_errors]
)å½“å·¥å…·å¤±è´¥æ—¶ï¼Œæ™ºèƒ½ä½“å°†è¿”å›å¸¦æœ‰è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯çš„ ToolMessageï¼š[
    ...
    ToolMessage(
        content="å·¥å…·é”™è¯¯ï¼šè¯·æ£€æŸ¥æ‚¨çš„è¾“å…¥å¹¶é‡è¯•ã€‚(division by zero)",
        tool_call_id="..."
    ),
    ...
]ReAct å¾ªç¯ä¸­çš„å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“éµå¾ª ReActï¼ˆâ€œæ¨ç† + è¡ŒåŠ¨â€ï¼‰æ¨¡å¼ï¼Œåœ¨ç®€çŸ­çš„æ¨ç†æ­¥éª¤ä¸é’ˆå¯¹æ€§çš„å·¥å…·è°ƒç”¨ä¹‹é—´äº¤æ›¿ï¼Œå¹¶å°†ç»“æœè§‚å¯Ÿåé¦ˆåˆ°åç»­å†³ç­–ä¸­ï¼Œç›´åˆ°èƒ½å¤Ÿæä¾›æœ€ç»ˆç­”æ¡ˆã€‚æç¤º è¦äº†è§£æ›´å¤šå…³äºå·¥å…·çš„ä¿¡æ¯ï¼Œè¯·å‚é˜… å·¥å…·ã€‚ç³»ç»Ÿæç¤ºæ‚¨å¯ä»¥é€šè¿‡æä¾›æç¤ºæ¥å¡‘é€ æ™ºèƒ½ä½“å¤„ç†ä»»åŠ¡çš„æ–¹å¼ã€‚system_prompt å‚æ•°å¯ä»¥ä½œä¸ºå­—ç¬¦ä¸²æä¾›ï¼šagent = create_agent(
    model,
    tools,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·ç®€æ´å‡†ç¡®ã€‚"
)å½“æœªæä¾› system_prompt æ—¶ï¼Œæ™ºèƒ½ä½“å°†ç›´æ¥ä»æ¶ˆæ¯ä¸­æ¨æ–­å…¶ä»»åŠ¡ã€‚åŠ¨æ€ç³»ç»Ÿæç¤ºå¯¹äºéœ€è¦æ ¹æ®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡æˆ–æ™ºèƒ½ä½“çŠ¶æ€ä¿®æ”¹ç³»ç»Ÿæç¤ºçš„é«˜çº§ç”¨ä¾‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ä¸­é—´ä»¶ã€‚@dynamic_prompt è£…é¥°å™¨åˆ›å»ºä¸­é—´ä»¶ï¼Œæ ¹æ®æ¨¡å‹è¯·æ±‚åŠ¨æ€ç”Ÿæˆç³»ç»Ÿæç¤ºï¼šfrom typing import TypedDict

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest


class Context(TypedDict):
    user_role: str

@dynamic_prompt
def user_role_prompt(request: ModelRequest) -> str:
    """æ ¹æ®ç”¨æˆ·è§’è‰²ç”Ÿæˆç³»ç»Ÿæç¤ºã€‚"""
    user_role = request.runtime.context.get("user_role", "user")
    base_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"

    if user_role == "expert":
        return f"{base_prompt} æä¾›è¯¦ç»†çš„æŠ€æœ¯å“åº”ã€‚"
    elif user_role == "beginner":
        return f"{base_prompt} ç®€å•è§£é‡Šæ¦‚å¿µï¼Œé¿å…ä½¿ç”¨è¡Œè¯ã€‚"

    return base_prompt

agent = create_agent(
    model="openai:gpt-4o",
    tools=[web_search],
    middleware=[user_role_prompt],
    context_schema=Context
)

# ç³»ç»Ÿæç¤ºå°†æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è®¾ç½®
result = agent.invoke(
    {"messages": [{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}]},
    context={"user_role": "expert"}
)æç¤º æœ‰å…³æ¶ˆæ¯ç±»å‹å’Œæ ¼å¼åŒ–çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… æ¶ˆæ¯ã€‚æœ‰å…³å…¨é¢çš„ä¸­é—´ä»¶æ–‡æ¡£ï¼Œè¯·å‚é˜… ä¸­é—´ä»¶ã€‚è°ƒç”¨æ‚¨å¯ä»¥é€šè¿‡å‘å…¶ State ä¼ é€’æ›´æ–°æ¥è°ƒç”¨æ™ºèƒ½ä½“ã€‚æ‰€æœ‰æ™ºèƒ½ä½“åœ¨å…¶çŠ¶æ€ä¸­åŒ…å«æ¶ˆæ¯åºåˆ—ï¼›è¦è°ƒç”¨æ™ºèƒ½ä½“ï¼Œè¯·ä¼ é€’ä¸€æ¡æ–°æ¶ˆæ¯ï¼šresult = agent.invoke(
    {"messages": [{"role": "user", "content": "æ—§é‡‘å±±å¤©æ°”å¦‚ä½•ï¼Ÿ"}]}
)æœ‰å…³ä»æ™ºèƒ½ä½“æµå¼ä¼ è¾“æ­¥éª¤å’Œ/æˆ–ä»¤ç‰Œçš„ä¿¡æ¯ï¼Œè¯·å‚é˜…æµå¼ä¼ è¾“æŒ‡å—ã€‚å¦åˆ™ï¼Œæ™ºèƒ½ä½“éµå¾ª LangGraph Graph API å¹¶æ”¯æŒæ‰€æœ‰ç›¸å…³æ–¹æ³•ã€‚é«˜çº§æ¦‚å¿µç»“æ„åŒ–è¾“å‡ºåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ™ºèƒ½ä½“ä»¥ç‰¹å®šæ ¼å¼è¿”å›è¾“å‡ºã€‚LangChain é€šè¿‡ response_format å‚æ•°æä¾›ç»“æ„åŒ–è¾“å‡ºç­–ç•¥ã€‚ToolStrategyToolStrategy ä½¿ç”¨äººå·¥å·¥å…·è°ƒç”¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºã€‚è¿™é€‚ç”¨äºä»»ä½•æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹ï¼šfrom pydantic import BaseModel
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class ContactInfo(BaseModel):
    name: str
    email: str
    phone: str

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[search_tool],
    response_format=ToolStrategy(ContactInfo)
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "ä»ä»¥ä¸‹å†…å®¹æå–è”ç³»ä¿¡æ¯ï¼šJohn Doe, john@example.com, (555) 123-4567"}]
})

result["structured_response"]
# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')ProviderStrategyProviderStrategy ä½¿ç”¨æ¨¡å‹æä¾›å•†çš„åŸç”Ÿç»“æ„åŒ–è¾“å‡ºç”Ÿæˆã€‚è¿™æ›´å¯é ï¼Œä½†ä»…é€‚ç”¨äºæ”¯æŒåŸç”Ÿç»“æ„åŒ–è¾“å‡ºçš„æä¾›å•†ï¼ˆä¾‹å¦‚ OpenAIï¼‰ï¼šfrom langchain.agents.structured_output import ProviderStrategy

agent = create_agent(
    model="openai:gpt-4o",
    response_format=ProviderStrategy(ContactInfo)
)æ³¨æ„ ä» langchain 1.0 å¼€å§‹ï¼Œä¸å†æ”¯æŒç›´æ¥ä¼ é€’æ¨¡å¼ï¼ˆä¾‹å¦‚ response_format=ContactInfoï¼‰ã€‚æ‚¨å¿…é¡»æ˜ç¡®ä½¿ç”¨ ToolStrategy æˆ– ProviderStrategyã€‚æç¤º è¦äº†è§£ç»“æ„åŒ–è¾“å‡ºï¼Œè¯·å‚é˜… ç»“æ„åŒ–è¾“å‡ºã€‚è®°å¿†æ™ºèƒ½ä½“é€šè¿‡æ¶ˆæ¯çŠ¶æ€è‡ªåŠ¨ç»´æŠ¤å¯¹è¯å†å²ã€‚æ‚¨è¿˜å¯ä»¥é…ç½®æ™ºèƒ½ä½“ä½¿ç”¨è‡ªå®šä¹‰çŠ¶æ€æ¨¡å¼ï¼Œåœ¨å¯¹è¯æœŸé—´è®°ä½é¢å¤–ä¿¡æ¯ã€‚å­˜å‚¨åœ¨çŠ¶æ€ä¸­çš„ä¿¡æ¯å¯ä»¥è¢«è§†ä¸ºæ™ºèƒ½ä½“çš„çŸ­æœŸè®°å¿†ï¼šè‡ªå®šä¹‰çŠ¶æ€æ¨¡å¼å¿…é¡»æ‰©å±• AgentState ä½œä¸º TypedDictã€‚å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€æœ‰ä¸¤ç§æ–¹å¼ï¼šé€šè¿‡ ä¸­é—´ä»¶ï¼ˆæ¨èï¼‰é€šè¿‡ create_agent ä¸Šçš„ state_schemaæ³¨æ„ æ¨èé€šè¿‡ä¸­é—´ä»¶å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€ï¼Œè€Œä¸æ˜¯é€šè¿‡ create_agent ä¸Šçš„ state_schemaï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨å°†çŠ¶æ€æ‰©å±•æ¦‚å¿µä¸Šé™å®šåœ¨ç›¸å…³ä¸­é—´ä»¶å’Œå·¥å…·çš„èŒƒå›´å†…ã€‚state_schema ä»æ”¯æŒç”¨äºå‘åå…¼å®¹ï¼Œåœ¨ create_agent ä¸Šã€‚é€šè¿‡ä¸­é—´ä»¶å®šä¹‰çŠ¶æ€å½“æ‚¨çš„è‡ªå®šä¹‰çŠ¶æ€éœ€è¦è¢«ç‰¹å®šä¸­é—´ä»¶é’©å­å’Œé™„åŠ åˆ°è¯¥ä¸­é—´ä»¶çš„å·¥å…·è®¿é—®æ—¶ï¼Œä½¿ç”¨ä¸­é—´ä»¶å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€ã€‚from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware


class CustomState(AgentState):
    user_preferences: dict

class CustomMiddleware(AgentMiddleware):
    state_schema = CustomState
    tools = [tool1, tool2]

    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        ...

agent = create_agent(
    model,
    tools=tools,
    middleware=[CustomMiddleware()]
)

# æ™ºèƒ½ä½“ç°åœ¨å¯ä»¥è·Ÿè¸ªæ¶ˆæ¯ä¹‹å¤–çš„é¢å¤–çŠ¶æ€
result = agent.invoke({
    "messages": [{"role": "user", "content": "æˆ‘æ›´å–œæ¬¢æŠ€æœ¯æ€§è§£é‡Š"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})é€šè¿‡ state_schema å®šä¹‰çŠ¶æ€ä½¿ç”¨ state_schema å‚æ•°ä½œä¸ºå¿«æ·æ–¹å¼ï¼Œå®šä¹‰ä»…åœ¨å·¥å…·ä¸­ä½¿ç”¨çš„è‡ªå®šä¹‰çŠ¶æ€ã€‚from langchain.agents import AgentState


class CustomState(AgentState):
    user_preferences: dict

agent = create_agent(
    model,
    tools=[tool1, tool2],
    state_schema=CustomState
)
# æ™ºèƒ½ä½“ç°åœ¨å¯ä»¥è·Ÿè¸ªæ¶ˆæ¯ä¹‹å¤–çš„é¢å¤–çŠ¶æ€
result = agent.invoke({
    "messages": [{"role": "user", "content": "æˆ‘æ›´å–œæ¬¢æŠ€æœ¯æ€§è§£é‡Š"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})æ³¨æ„ ä» langchain 1.0 å¼€å§‹ï¼Œè‡ªå®šä¹‰çŠ¶æ€æ¨¡å¼å¿…é¡»æ˜¯ TypedDict ç±»å‹ã€‚ä¸å†æ”¯æŒ Pydantic æ¨¡å‹å’Œæ•°æ®ç±»ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… v1 è¿ç§»æŒ‡å—ã€‚æç¤º è¦äº†è§£æ›´å¤šå…³äºè®°å¿†çš„ä¿¡æ¯ï¼Œè¯·å‚é˜… è®°å¿†ã€‚æœ‰å…³å®ç°è·¨ä¼šè¯æŒä¹…åŒ–çš„é•¿æœŸè®°å¿†çš„ä¿¡æ¯ï¼Œè¯·å‚é˜… é•¿æœŸè®°å¿†ã€‚æµå¼ä¼ è¾“æˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ invoke è°ƒç”¨æ™ºèƒ½ä½“ä»¥è·å–æœ€ç»ˆå“åº”ã€‚å¦‚æœæ™ºèƒ½ä½“æ‰§è¡Œå¤šä¸ªæ­¥éª¤ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚ä¸ºäº†æ˜¾ç¤ºä¸­é—´è¿›åº¦ï¼Œæˆ‘ä»¬å¯ä»¥éšç€æ¶ˆæ¯çš„å‘ç”Ÿè€Œæµå¼ä¼ å›ã€‚for chunk in agent.stream({
    "messages": [{"role": "user", "content": "æœç´¢ AI æ–°é—»å¹¶æ€»ç»“å‘ç°"}]
}, stream_mode="values"):
    # æ¯ä¸ªå—åŒ…å«è¯¥æ—¶é—´ç‚¹çš„å®Œæ•´çŠ¶æ€
    latest_message = chunk["messages"][-1]
    if latest_message.content:
        print(f"æ™ºèƒ½ä½“ï¼š{latest_message.content}")
    elif latest_message.tool_calls:
        print(f"æ­£åœ¨è°ƒç”¨å·¥å…·ï¼š{[tc['name'] for tc in latest_message.tool_calls]}")æç¤º æœ‰å…³æµå¼ä¼ è¾“çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… æµå¼ä¼ è¾“ã€‚ä¸­é—´ä»¶ä¸­é—´ä»¶ ä¸ºåœ¨æ‰§è¡Œçš„ä¸åŒé˜¶æ®µè‡ªå®šä¹‰æ™ºèƒ½ä½“è¡Œä¸ºæä¾›äº†å¼ºå¤§çš„æ‰©å±•æ€§ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸­é—´ä»¶æ¥ï¼šåœ¨è°ƒç”¨æ¨¡å‹ä¹‹å‰å¤„ç†çŠ¶æ€ï¼ˆä¾‹å¦‚æ¶ˆæ¯è£å‰ªã€ä¸Šä¸‹æ–‡æ³¨å…¥ï¼‰ä¿®æ”¹æˆ–éªŒè¯æ¨¡å‹çš„å“åº”ï¼ˆä¾‹å¦‚é˜²æŠ¤æ ã€å†…å®¹è¿‡æ»¤ï¼‰ä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘å¤„ç†å·¥å…·æ‰§è¡Œé”™è¯¯åŸºäºçŠ¶æ€æˆ–ä¸Šä¸‹æ–‡å®ç°åŠ¨æ€æ¨¡å‹é€‰æ‹©æ·»åŠ è‡ªå®šä¹‰æ—¥å¿—ã€ç›‘æ§æˆ–åˆ†æä¸­é—´ä»¶æ— ç¼é›†æˆåˆ°æ™ºèƒ½ä½“çš„æ‰§è¡Œå›¾ä¸­ï¼Œå…è®¸æ‚¨åœ¨å…³é”®ç‚¹æ‹¦æˆªå’Œä¿®æ”¹æ•°æ®æµï¼Œè€Œæ— éœ€æ›´æ”¹æ ¸å¿ƒæ™ºèƒ½ä½“é€»è¾‘ã€‚æç¤º æœ‰å…³å…¨é¢çš„ä¸­é—´ä»¶æ–‡æ¡£ï¼ŒåŒ…æ‹¬è£…é¥°å™¨å¦‚ @before_modelã€@after_model å’Œ @wrap_tool_callï¼Œè¯·å‚é˜… ä¸­é—´ä»¶ã€‚æœ€è¿‘æ›´æ–°2025/10/25 19:51ä¸‹ä¸€é¡µæ¨¡å‹

---
